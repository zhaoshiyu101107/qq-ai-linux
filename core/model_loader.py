"""
æ¨¡å‹åŠ è½½æ¨¡å—
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import Tuple, Dict, Any
from config.model_config import get_model_config
from config.gpu_config import load_gpu_config

class ModelLoader:
    """æ¨¡å‹åŠ è½½å™¨"""
    
    def __init__(self, model_key: str = 'qwen2.5-0.5b'):
        self.model_key = model_key
        self.model_config = get_model_config(model_key)
        self.gpu_config = load_gpu_config()
        self.tokenizer = None
        self.model = None
        
    def load(self) -> Tuple[Any, Any]:
        """åŠ è½½æ¨¡å‹å’Œtokenizer"""
        print(f"ğŸ¤– åŠ è½½æ¨¡å‹: {self.model_config['name']}")
        
        # åŠ è½½tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_config['name'],
            trust_remote_code=self.model_config['trust_remote_code'],
            token=self.model_config['token']
        )
        
        # ç¡®å®šæ•°æ®ç±»å‹
        torch_dtype = self._get_torch_dtype()
        
        # åŠ è½½æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_config['name'],
            trust_remote_code=self.model_config['trust_remote_code'],
            token=self.model_config['token'],
            torch_dtype=torch_dtype,
            device_map=self.gpu_config['device_map']
        )
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        print(f"   è®¾å¤‡: {self.model.device}")
        print(f"   æ•°æ®ç±»å‹: {self.model.dtype}")
        
        return self.tokenizer, self.model
    
    def _get_torch_dtype(self):
        """è·å–PyTorchæ•°æ®ç±»å‹"""
        dtype_str = self.gpu_config.get('torch_dtype', 'float32')
        
        if dtype_str == 'float16':
            return torch.float16
        elif dtype_str == 'bfloat16':
            return torch.bfloat16
        else:
            return torch.float32
    
    def get_memory_usage(self) -> float:
        """è·å–æ¨¡å‹å†…å­˜ä½¿ç”¨é‡ï¼ˆGBï¼‰"""
        if self.model is None:
            return 0.0
        
        param_count = sum(p.numel() for p in self.model.parameters())
        
        # æ ¹æ®æ•°æ®ç±»å‹è®¡ç®—å†…å­˜
        if self.model.dtype == torch.float16:
            bytes_per_param = 2
        elif self.model.dtype == torch.bfloat16:
            bytes_per_param = 2
        else:
            bytes_per_param = 4
        
        memory_gb = (param_count * bytes_per_param) / (1024**3)
        return memory_gb
    
    def unload(self):
        """å¸è½½æ¨¡å‹é‡Šæ”¾å†…å­˜"""
        if self.model is not None:
            del self.model
            self.model = None
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
            print("ğŸ—‘ï¸  æ¨¡å‹å·²å¸è½½")