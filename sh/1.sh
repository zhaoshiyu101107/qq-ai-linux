#!/bin/bash
# è‡ªåŒ…å«AIçŽ¯å¢ƒå®‰è£…è„šæœ¬ - æ‰€æœ‰å†…å®¹å®‰è£…åˆ°è„šæœ¬æ‰€åœ¨ç›®å½•

set -e

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m'

echo -e "${GREEN}ðŸš€ å¼€å§‹åˆ›å»ºè‡ªåŒ…å«AIçŽ¯å¢ƒ...${NC}"

# èŽ·å–è„šæœ¬æ‰€åœ¨ç›®å½•
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
AI_HOME="${SCRIPT_DIR}/ai_env"

echo "å®‰è£…ç›®å½•: ${AI_HOME}"

# åˆ›å»ºç›®å½•ç»“æž„
mkdir -p "${AI_HOME}"
cd "${AI_HOME}"

# åˆ›å»ºè™šæ‹ŸçŽ¯å¢ƒï¼ˆéš”ç¦»çŽ¯å¢ƒï¼‰
echo -e "\n${YELLOW}[1/4] åˆ›å»ºPythonè™šæ‹ŸçŽ¯å¢ƒ...${NC}"
python3 -m venv venv 2>/dev/null || python -m venv venv

# æ¿€æ´»è™šæ‹ŸçŽ¯å¢ƒ
source venv/bin/activate

# å‡çº§pip
echo -e "\n${YELLOW}[2/4] å®‰è£…Pythonä¾èµ–...${NC}"
pip install --upgrade pip > /dev/null 2>&1

# å®‰è£…PyTorchï¼ˆCPUç‰ˆæœ¬ï¼Œç¨³å®šä¸”ä½“ç§¯å°ï¼‰
echo "å®‰è£…PyTorch..."
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu > /dev/null 2>&1

# å®‰è£…Transformerså’Œå…¶ä»–AIåº“
echo "å®‰è£…Transformers..."
pip install transformers accelerate sentencepiece protobuf einops tiktoken > /dev/null 2>&1

# åˆ›å»ºé…ç½®æ–‡ä»¶
echo -e "\n${YELLOW}[3/4] åˆ›å»ºé…ç½®æ–‡ä»¶...${NC}"

cat > config.json << 'EOF'
{
  "environment": "local",
  "model": "Qwen/Qwen3-0.6B-Instruct",
  "install_date": "$(date)",
  "install_dir": "${AI_HOME}",
  "requirements": [
    "torch",
    "transformers",
    "accelerate",
    "sentencepiece",
    "protobuf",
    "einops",
    "tiktoken"
  ]
}
EOF

# åˆ›å»ºå¯åŠ¨è„šæœ¬
echo -e "\n${YELLOW}[4/4] åˆ›å»ºå¯åŠ¨è„šæœ¬...${NC}"

cat > start_qwen.sh << 'EOF'
#!/bin/bash
# Qwen3-0.6B å¯åŠ¨è„šæœ¬

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
cd "$SCRIPT_DIR"

echo "=========================================="
echo "ðŸ¤– Qwen3-0.6B æœ¬åœ°è¿è¡ŒçŽ¯å¢ƒ"
echo "=========================================="
echo "ä½ç½®: $SCRIPT_DIR"
echo ""

# æ£€æŸ¥æ˜¯å¦åœ¨è™šæ‹ŸçŽ¯å¢ƒä¸­
if [ -z "$VIRTUAL_ENV" ]; then
    echo "æ¿€æ´»è™šæ‹ŸçŽ¯å¢ƒ..."
    if [ -f "venv/bin/activate" ]; then
        source venv/bin/activate
    else
        echo "é”™è¯¯: æœªæ‰¾åˆ°è™šæ‹ŸçŽ¯å¢ƒ"
        exit 1
    fi
fi

# è¿è¡ŒPythonè„šæœ¬
python run_qwen.py "$@"
EOF

chmod +x start_qwen.sh

# åˆ›å»ºä¸»è¿è¡Œè„šæœ¬
cat > run_qwen.py << 'EOF'
#!/usr/bin/env python3
"""
Qwen3-0.6B æœ¬åœ°è¿è¡Œè„šæœ¬
æ— éœ€ç½‘ç»œè¿žæŽ¥ï¼Œå®Œå…¨æœ¬åœ°è¿è¡Œ
"""

import os
import sys
import torch
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

print("=" * 50)
print("ðŸ¤– Qwen3-0.6B - æœ¬åœ°AIçŽ¯å¢ƒ")
print("=" * 50)
print(f"ç›®å½•: {os.path.dirname(os.path.abspath(__file__))}")
print(f"Python: {sys.version}")
print(f"PyTorch: {torch.__version__}")
print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
print("=" * 50)
print()

def main():
    from transformers import AutoTokenizer, AutoModelForCausalLM
    
    print("æ­£åœ¨åŠ è½½Qwen3-0.6Bæ¨¡åž‹...")
    print("é¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½æ¨¡åž‹æ–‡ä»¶ï¼ˆçº¦1.2GBï¼‰")
    print("ä¸‹è½½å®ŒæˆåŽä¼šç¼“å­˜ï¼Œä¸‹æ¬¡æ— éœ€é‡æ–°ä¸‹è½½")
    print()
    
    # æ¨¡åž‹åç§°
    model_name = "Qwen/Qwen3-0.6B-Instruct"
    
    # è®¾ç½®æ¨¡åž‹ç¼“å­˜ç›®å½•åˆ°æœ¬åœ°
    cache_dir = os.path.join(os.path.dirname(__file__), "model_cache")
    os.makedirs(cache_dir, exist_ok=True)
    
    try:
        # åŠ è½½tokenizer
        print("1. åŠ è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            trust_remote_code=True
        )
        
        # åŠ è½½æ¨¡åž‹
        print("2. åŠ è½½æ¨¡åž‹...")
        if torch.cuda.is_available():
            print("  ä½¿ç”¨GPUåŠ é€Ÿ")
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16,
                device_map="auto",
                cache_dir=cache_dir,
                trust_remote_code=True
            )
        else:
            print("  ä½¿ç”¨CPUè¿è¡Œ")
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float32,
                device_map="cpu",
                cache_dir=cache_dir,
                trust_remote_code=True
            )
        
        print()
        print("âœ… æ¨¡åž‹åŠ è½½æˆåŠŸï¼")
        print("-" * 40)
        print("ðŸ’¡ æç¤º: è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
        print("-" * 40)
        
        while True:
            try:
                user_input = input("\nä½ : ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'q']:
                    print("å†è§ï¼ðŸ‘‹")
                    break
                    
                if not user_input:
                    continue
                
                # å‡†å¤‡å¯¹è¯æ ¼å¼
                messages = [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¹äºŽåŠ©äººçš„AIåŠ©æ‰‹ã€‚"},
                    {"role": "user", "content": user_input}
                ]
                
                text = tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
                
                # ç¼–ç å¹¶ç”Ÿæˆ
                inputs = tokenizer(text, return_tensors="pt").to(model.device)
                
                print("æ€è€ƒä¸­...", end="", flush=True)
                
                with torch.no_grad():
                    outputs = model.generate(
                        **inputs,
                        max_new_tokens=200,
                        temperature=0.7,
                        do_sample=True,
                        pad_token_id=tokenizer.eos_token_id
                    )
                
                print("\r" + " " * 20, end="\r")  # æ¸…é™¤"æ€è€ƒä¸­..."
                
                # è§£ç å›žå¤
                response = tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                # æå–æ¨¡åž‹å›žå¤éƒ¨åˆ†
                if "assistant" in response:
                    response = response.split("assistant")[-1].strip()
                elif "Assistant:" in response:
                    response = response.split("Assistant:")[-1].strip()
                
                print(f"Qwen: {response}")
                
            except KeyboardInterrupt:
                print("\n\né€€å‡ºç¨‹åº")
                break
            except Exception as e:
                print(f"\né”™è¯¯: {e}")
                continue
                
    except Exception as e:
        print(f"\nâŒ åŠ è½½æ¨¡åž‹å¤±è´¥: {e}")
        print("\nå¯èƒ½çš„åŽŸå› :")
        print("1. ç½‘ç»œè¿žæŽ¥é—®é¢˜")
        print("2. ç£ç›˜ç©ºé—´ä¸è¶³")
        print("3. å†…å­˜ä¸è¶³")
        print("\nðŸ’¡ è§£å†³æ–¹æ¡ˆ:")
        print(f"   æ£€æŸ¥ç›®å½•: {cache_dir}")
        print("   ç¡®ä¿æœ‰è‡³å°‘2GBå¯ç”¨ç©ºé—´")
        print("   æ£€æŸ¥ç½‘ç»œè¿žæŽ¥")

if __name__ == "__main__":
    main()
EOF

# åˆ›å»ºå·¥å…·è„šæœ¬
cat > tools.py << 'EOF'
#!/usr/bin/env python3
"""
AIçŽ¯å¢ƒå·¥å…·è„šæœ¬
"""

import torch
import sys
import os

def check_environment():
    """æ£€æŸ¥çŽ¯å¢ƒçŠ¶æ€"""
    print("ðŸ” çŽ¯å¢ƒæ£€æŸ¥")
    print("-" * 40)
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}")
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
    
    # æ£€æŸ¥å…¶ä»–åŒ…
    packages = [
        ("transformers", "transformers"),
        ("accelerate", "accelerate"),
        ("sentencepiece", "sentencepiece"),
        ("einops", "einops"),
    ]
    
    for name, module in packages:
        try:
            __import__(module)
            print(f"âœ… {name}: å·²å®‰è£…")
        except ImportError:
            print(f"âŒ {name}: æœªå®‰è£…")
    
    print("-" * 40)

def clear_cache():
    """æ¸…ç†æ¨¡åž‹ç¼“å­˜"""
    import shutil
    cache_dir = os.path.join(os.path.dirname(__file__), "model_cache")
    
    if os.path.exists(cache_dir):
        size = sum(os.path.getsize(os.path.join(cache_dir, f)) 
                   for f in os.listdir(cache_dir) 
                   if os.path.isfile(os.path.join(cache_dir, f))) / (1024**3)
        
        print(f"ç¼“å­˜å¤§å°: {size:.2f} GB")
        response = input("ç¡®è®¤åˆ é™¤ç¼“å­˜ï¼Ÿ(y/N): ").strip().lower()
        
        if response == 'y':
            shutil.rmtree(cache_dir)
            print("âœ… ç¼“å­˜å·²æ¸…ç†")
        else:
            print("âŒ å–æ¶ˆæ“ä½œ")
    else:
        print("âœ… ç¼“å­˜ç›®å½•ä¸å­˜åœ¨")

if __name__ == "__main__":
    if len(sys.argv) > 1:
        if sys.argv[1] == "check":
            check_environment()
        elif sys.argv[1] == "clear":
            clear_cache()
        else:
            print("ç”¨æ³•: python tools.py [check|clear]")
    else:
        check_environment()
EOF

# åˆ›å»ºREADMEæ–‡ä»¶
cat > README.md << 'EOF'
# ðŸ¤– æœ¬åœ°AIçŽ¯å¢ƒ

è¿™æ˜¯ä¸€ä¸ªå®Œå…¨è‡ªåŒ…å«çš„AIè¿è¡ŒçŽ¯å¢ƒï¼ŒåŒ…å«Qwen3-0.6Bæ¨¡åž‹ã€‚

## ç›®å½•ç»“æž„