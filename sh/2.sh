#!/bin/bash
# è‡ªåŒ…å«AIç¯å¢ƒå®‰è£…è„šæœ¬ - æ‰€æœ‰å†…å®¹å®‰è£…åˆ°è„šæœ¬æ‰€åœ¨ç›®å½•

set -e

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

echo -e "${GREEN}ğŸš€ å¼€å§‹åˆ›å»ºè‡ªåŒ…å«AIç¯å¢ƒ...${NC}"

# è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
AI_HOME="${SCRIPT_DIR}/ai_env"

echo "å®‰è£…ç›®å½•: ${AI_HOME}"

# åˆ›å»ºç›®å½•ç»“æ„
mkdir -p "${AI_HOME}"
cd "${AI_HOME}"

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆéš”ç¦»ç¯å¢ƒï¼‰
echo -e "\n${YELLOW}[1/5] åˆ›å»ºPythonè™šæ‹Ÿç¯å¢ƒ...${NC}"
python3 -m venv venv 2>/dev/null || python -m venv venv

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate

# å‡çº§pip
echo -e "\n${YELLOW}[2/5] å‡çº§pip...${NC}"
pip install --upgrade pip > /dev/null 2>&1

# æ£€æµ‹GPU
echo -e "\n${YELLOW}[3/5] æ£€æµ‹ç³»ç»Ÿç¯å¢ƒ...${NC}"

# æ£€æµ‹NVIDIA GPU
HAS_CUDA=false
HAS_NVIDIA=false
PYTORCH_VERSION="cpu"

# æ£€æŸ¥nvidia-smiå‘½ä»¤
if command -v nvidia-smi &> /dev/null; then
    HAS_NVIDIA=true
    echo -e "${GREEN}âœ“ æ£€æµ‹åˆ°NVIDIA GPU${NC}"
    
    # æ£€æŸ¥CUDAç‰ˆæœ¬
    if nvidia-smi | grep -q "CUDA Version"; then
        HAS_CUDA=true
        CUDA_VERSION=$(nvidia-smi | grep "CUDA Version" | awk '{print $9}' | cut -d'.' -f1)
        echo -e "${GREEN}âœ“ æ£€æµ‹åˆ°CUDAç‰ˆæœ¬: ${CUDA_VERSION}.x${NC}"
    else
        echo -e "${YELLOW}âš  NVIDIAé©±åŠ¨å·²å®‰è£…ä½†æœªæ£€æµ‹åˆ°CUDA${NC}"
    fi
else
    echo -e "${YELLOW}â„¹ æœªæ£€æµ‹åˆ°NVIDIA GPUæˆ–é©±åŠ¨${NC}"
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–GPU
    if command -v lspci &> /dev/null; then
        if lspci | grep -i "vga\|3d\|display" | grep -v "NVIDIA"; then
            echo -e "${YELLOW}âš  æ£€æµ‹åˆ°å…¶ä»–æ˜¾å¡ï¼ˆAMD/Intelï¼‰ï¼Œä»…æ”¯æŒCPUæ¨¡å¼${NC}"
        fi
    fi
fi

# è¯¢é—®ç”¨æˆ·é€‰æ‹©PyTorchç‰ˆæœ¬
echo -e "\n${BLUE}è¯·é€‰æ‹©PyTorchå®‰è£…ç‰ˆæœ¬:${NC}"
if [ "$HAS_NVIDIA" = true ] && [ "$HAS_CUDA" = true ]; then
    echo "1. GPUåŠ é€Ÿç‰ˆ (CUDA ${CUDA_VERSION}.x) - æ¨èï¼Œéœ€è¦NVIDIA GPU"
    echo "2. CPUç‰ˆ - é€šç”¨å…¼å®¹ï¼Œæ— GPUåŠ é€Ÿ"
    echo "3. CPU+GPUç‰ˆ - åŒæ—¶å®‰è£…CPUå’ŒGPUæ”¯æŒ"
    
    read -p "è¯·é€‰æ‹© [1/2/3] (é»˜è®¤: 1): " choice
    
    case $choice in
        2)
            PYTORCH_VERSION="cpu"
            echo -e "${YELLOW}é€‰æ‹©: CPUç‰ˆæœ¬${NC}"
            ;;
        3)
            PYTORCH_VERSION="both"
            echo -e "${YELLOW}é€‰æ‹©: CPU+GPUç‰ˆæœ¬${NC}"
            ;;
        *)
            PYTORCH_VERSION="gpu"
            echo -e "${YELLOW}é€‰æ‹©: GPUåŠ é€Ÿç‰ˆæœ¬${NC}"
            ;;
    esac
elif [ "$HAS_NVIDIA" = true ] && [ "$HAS_CUDA" = false ]; then
    echo "1. CPUç‰ˆ - NVIDIAé©±åŠ¨å·²å®‰è£…ä½†CUDAä¸å¯ç”¨"
    echo "2. å°è¯•å®‰è£…GPUç‰ˆ - å¯èƒ½éœ€è¦é¢å¤–é…ç½®CUDA"
    
    read -p "è¯·é€‰æ‹© [1/2] (é»˜è®¤: 1): " choice
    
    if [ "$choice" = "2" ]; then
        PYTORCH_VERSION="gpu"
        echo -e "${YELLOW}é€‰æ‹©: å°è¯•å®‰è£…GPUç‰ˆæœ¬${NC}"
        echo -e "${YELLOW}æ³¨æ„: å¦‚æœCUDAæœªæ­£ç¡®å®‰è£…ï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨å®‰è£…CUDAå·¥å…·åŒ…${NC}"
    else
        PYTORCH_VERSION="cpu"
        echo -e "${YELLOW}é€‰æ‹©: CPUç‰ˆæœ¬${NC}"
    fi
else
    echo "1. CPUç‰ˆ - å”¯ä¸€å¯ç”¨é€‰é¡¹"
    PYTORCH_VERSION="cpu"
    echo -e "${YELLOW}é€‰æ‹©: CPUç‰ˆæœ¬${NC}"
fi

# æ ¹æ®é€‰æ‹©å®‰è£…PyTorch
echo -e "\n${YELLOW}[4/5] å®‰è£…PyTorch...${NC}"

case $PYTORCH_VERSION in
    "cpu")
        echo "å®‰è£…PyTorch CPUç‰ˆæœ¬..."
        pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu > /dev/null 2>&1
        ;;
    "gpu")
        echo "å®‰è£…PyTorch GPUç‰ˆæœ¬..."
        
        # æ ¹æ®æ£€æµ‹åˆ°çš„CUDAç‰ˆæœ¬é€‰æ‹©å¯¹åº”çš„PyTorch
        if [ "$CUDA_VERSION" = "12" ] || [ "$CUDA_VERSION" = "12.1" ] || [ "$CUDA_VERSION" = "12.2" ] || [ "$CUDA_VERSION" = "12.3" ] || [ "$CUDA_VERSION" = "12.4" ]; then
            echo "ä½¿ç”¨CUDA 12.1ç‰ˆæœ¬..."
            pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 > /dev/null 2>&1
        elif [ "$CUDA_VERSION" = "11" ] || [ "$CUDA_VERSION" = "11.8" ]; then
            echo "ä½¿ç”¨CUDA 11.8ç‰ˆæœ¬..."
            pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 > /dev/null 2>&1
        else
            echo "ä½¿ç”¨é»˜è®¤CUDAç‰ˆæœ¬ (11.8)..."
            pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 > /dev/null 2>&1
        fi
        ;;
    "both")
        echo "å®‰è£…PyTorch CPU+GPUç‰ˆæœ¬..."
        echo "æ³¨æ„: è¿™å°†å®‰è£…è¾ƒå¤§çš„åŒ…ï¼ŒåŒæ—¶æ”¯æŒCPUå’ŒGPUè¿è¡Œ"
        pip install torch torchvision torchaudio > /dev/null 2>&1
        ;;
esac

# å®‰è£…Transformerså’Œå…¶ä»–AIåº“
echo "å®‰è£…Transformerså’Œå…¶ä»–AIåº“..."
pip install transformers accelerate sentencepiece protobuf einops tiktoken > /dev/null 2>&1

# åˆ›å»ºé…ç½®æ–‡ä»¶
echo -e "\n${YELLOW}[5/5] åˆ›å»ºé…ç½®æ–‡ä»¶...${NC}"

# è·å–å½“å‰æ—¥æœŸ
CURRENT_DATE=$(date '+%Y-%m-%d %H:%M:%S')

cat > config.json << EOF
{
  "environment": "local",
  "model": "Qwen/Qwen3-0.6B-Instruct",
  "install_date": "${CURRENT_DATE}",
  "install_dir": "${AI_HOME}",
  "pytorch_version": "${PYTORCH_VERSION}",
  "has_gpu": ${HAS_NVIDIA},
  "has_cuda": ${HAS_CUDA},
  "cuda_version": "${CUDA_VERSION:-null}",
  "requirements": [
    "torch",
    "transformers",
    "accelerate",
    "sentencepiece",
    "protobuf",
    "einops",
    "tiktoken"
  ]
}
EOF

# åˆ›å»ºå¯åŠ¨è„šæœ¬
echo "åˆ›å»ºå¯åŠ¨è„šæœ¬..."

cat > start_qwen.sh << 'EOF'
#!/bin/bash
# Qwen3-0.6B å¯åŠ¨è„šæœ¬

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
cd "$SCRIPT_DIR"

echo "=========================================="
echo "ğŸ¤– Qwen3-0.6B æœ¬åœ°è¿è¡Œç¯å¢ƒ"
echo "=========================================="
echo "ä½ç½®: $SCRIPT_DIR"
echo ""

# æ£€æŸ¥æ˜¯å¦åœ¨è™šæ‹Ÿç¯å¢ƒä¸­
if [ -z "$VIRTUAL_ENV" ]; then
    echo "æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ..."
    if [ -f "venv/bin/activate" ]; then
        source venv/bin/activate
    else
        echo "é”™è¯¯: æœªæ‰¾åˆ°è™šæ‹Ÿç¯å¢ƒ"
        exit 1
    fi
fi

# è¿è¡ŒPythonè„šæœ¬
python run_qwen.py "$@"
EOF

chmod +x start_qwen.sh

# åˆ›å»ºä¸»è¿è¡Œè„šæœ¬
cat > run_qwen.py << 'EOF'
#!/usr/bin/env python3
"""
Qwen3-0.6B æœ¬åœ°è¿è¡Œè„šæœ¬
æ— éœ€ç½‘ç»œè¿æ¥ï¼Œå®Œå…¨æœ¬åœ°è¿è¡Œ
"""

import os
import sys
import torch
import json
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

def check_gpu_info():
    """æ£€æŸ¥GPUä¿¡æ¯"""
    print("=" * 50)
    print("ğŸ¤– Qwen3-0.6B - æœ¬åœ°AIç¯å¢ƒ")
    print("=" * 50)
    print(f"ç›®å½•: {os.path.dirname(os.path.abspath(__file__))}")
    print(f"Python: {sys.version}")
    print(f"PyTorch: {torch.__version__}")
    
    # æ£€æŸ¥CUDA
    cuda_available = torch.cuda.is_available()
    print(f"CUDAå¯ç”¨: {cuda_available}")
    
    if cuda_available:
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
            print(f"    å†…å­˜: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB")
    else:
        print("âš  ä½¿ç”¨CPUæ¨¡å¼ - é€Ÿåº¦è¾ƒæ…¢")
        print("ğŸ’¡ æç¤º: å¦‚éœ€GPUåŠ é€Ÿï¼Œè¯·ç¡®ä¿å·²å®‰è£…NVIDIAé©±åŠ¨å’ŒCUDA")
    
    print("=" * 50)
    print()

def main():
    from transformers import AutoTokenizer, AutoModelForCausalLM
    
    check_gpu_info()
    
    print("æ­£åœ¨åŠ è½½Qwen3-0.6Bæ¨¡å‹...")
    print("é¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼ˆçº¦1.2GBï¼‰")
    print("ä¸‹è½½å®Œæˆåä¼šç¼“å­˜ï¼Œä¸‹æ¬¡æ— éœ€é‡æ–°ä¸‹è½½")
    print()
    
    # æ¨¡å‹åç§°
    model_name = "Qwen/Qwen3-0.6B-Instruct"
    
    # è®¾ç½®æ¨¡å‹ç¼“å­˜ç›®å½•åˆ°æœ¬åœ°
    cache_dir = os.path.join(os.path.dirname(__file__), "model_cache")
    os.makedirs(cache_dir, exist_ok=True)
    
    try:
        # åŠ è½½tokenizer
        print("1. åŠ è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            trust_remote_code=True
        )
        
        # åŠ è½½æ¨¡å‹
        print("2. åŠ è½½æ¨¡å‹...")
        
        # æ ¹æ®æ˜¯å¦æœ‰GPUé€‰æ‹©åŠ è½½æ–¹å¼
        if torch.cuda.is_available():
            print("  ä½¿ç”¨GPUåŠ é€Ÿ")
            try:
                # å°è¯•ä½¿ç”¨GPU
                model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype=torch.float16,
                    device_map="auto",
                    cache_dir=cache_dir,
                    trust_remote_code=True
                )
            except Exception as e:
                print(f"  GPUåŠ è½½å¤±è´¥: {e}")
                print("  å›é€€åˆ°CPUæ¨¡å¼...")
                model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype=torch.float32,
                    device_map="cpu",
                    cache_dir=cache_dir,
                    trust_remote_code=True
                )
        else:
            print("  ä½¿ç”¨CPUè¿è¡Œ")
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float32,
                device_map="cpu",
                cache_dir=cache_dir,
                trust_remote_code=True
            )
        
        print()
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        print("-" * 40)
        print("ğŸ’¡ æç¤º: è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
        print("-" * 40)
        
        while True:
            try:
                user_input = input("\nä½ : ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'q']:
                    print("å†è§ï¼ğŸ‘‹")
                    break
                    
                if not user_input:
                    continue
                
                # å‡†å¤‡å¯¹è¯æ ¼å¼
                messages = [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„AIåŠ©æ‰‹ã€‚"},
                    {"role": "user", "content": user_input}
                ]
                
                text = tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
                
                # ç¼–ç å¹¶ç”Ÿæˆ
                model_inputs = tokenizer(text, return_tensors="pt").to(model.device)
                
                print("æ€è€ƒä¸­...", end="", flush=True)
                
                with torch.no_grad():
                    outputs = model.generate(
                        **model_inputs,
                        max_new_tokens=200,
                        temperature=0.7,
                        do_sample=True,
                        pad_token_id=tokenizer.eos_token_id
                    )
                
                print("\r" + " " * 20, end="\r")  # æ¸…é™¤"æ€è€ƒä¸­..."
                
                # è§£ç å›å¤
                response = tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                # æå–æ¨¡å‹å›å¤éƒ¨åˆ†
                if "assistant" in response:
                    response = response.split("assistant")[-1].strip()
                elif "Assistant:" in response:
                    response = response.split("Assistant:")[-1].strip()
                
                print(f"Qwen: {response}")
                
            except KeyboardInterrupt:
                print("\n\né€€å‡ºç¨‹åº")
                break
            except Exception as e:
                print(f"\né”™è¯¯: {e}")
                continue
                
    except Exception as e:
        print(f"\nâŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        print("\nå¯èƒ½çš„åŸå› :")
        print("1. ç½‘ç»œè¿æ¥é—®é¢˜")
        print("2. ç£ç›˜ç©ºé—´ä¸è¶³")
        print("3. å†…å­˜ä¸è¶³")
        print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
        print(f"   æ£€æŸ¥ç›®å½•: {cache_dir}")
        print("   ç¡®ä¿æœ‰è‡³å°‘2GBå¯ç”¨ç©ºé—´")
        print("   æ£€æŸ¥ç½‘ç»œè¿æ¥")

if __name__ == "__main__":
    main()
EOF

# åˆ›å»ºå·¥å…·è„šæœ¬
cat > tools.py << 'EOF'
#!/usr/bin/env python3
"""
AIç¯å¢ƒå·¥å…·è„šæœ¬
"""

import torch
import sys
import os
import json

def check_environment():
    """æ£€æŸ¥ç¯å¢ƒçŠ¶æ€"""
    print("ğŸ” ç¯å¢ƒæ£€æŸ¥")
    print("-" * 50)
    
    # è¯»å–é…ç½®æ–‡ä»¶
    config_path = os.path.join(os.path.dirname(__file__), "config.json")
    if os.path.exists(config_path):
        with open(config_path, 'r') as f:
            config = json.load(f)
        print(f"å®‰è£…æ—¥æœŸ: {config.get('install_date', 'æœªçŸ¥')}")
        print(f"PyTorchç‰ˆæœ¬: {config.get('pytorch_version', 'æœªçŸ¥')}")
        print(f"æ£€æµ‹åˆ°GPU: {config.get('has_gpu', False)}")
        if config.get('has_cuda'):
            print(f"CUDAç‰ˆæœ¬: {config.get('cuda_version', 'æœªçŸ¥')}")
    else:
        print("é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°")
    
    print("-" * 50)
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
            print(f"    å†…å­˜: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB")
    else:
        print("âš  å½“å‰è¿è¡Œåœ¨CPUæ¨¡å¼")
    
    # æ£€æŸ¥å…¶ä»–åŒ…
    packages = [
        ("transformers", "transformers"),
        ("accelerate", "accelerate"),
        ("sentencepiece", "sentencepiece"),
        ("einops", "einops"),
    ]
    
    print("-" * 50)
    for name, module in packages:
        try:
            __import__(module)
            print(f"âœ… {name}: å·²å®‰è£…")
        except ImportError:
            print(f"âŒ {name}: æœªå®‰è£…")
    
    print("-" * 50)

def clear_cache():
    """æ¸…ç†æ¨¡å‹ç¼“å­˜"""
    import shutil
    cache_dir = os.path.join(os.path.dirname(__file__), "model_cache")
    
    if os.path.exists(cache_dir):
        total_size = 0
        file_count = 0
        
        for root, dirs, files in os.walk(cache_dir):
            for file in files:
                file_path = os.path.join(root, file)
                total_size += os.path.getsize(file_path)
                file_count += 1
        
        size_gb = total_size / (1024**3)
        
        print(f"ç¼“å­˜æ–‡ä»¶: {file_count} ä¸ª")
        print(f"ç¼“å­˜å¤§å°: {size_gb:.2f} GB")
        print()
        
        response = input("ç¡®è®¤åˆ é™¤æ‰€æœ‰ç¼“å­˜æ–‡ä»¶ï¼Ÿ(y/N): ").strip().lower()
        
        if response == 'y':
            shutil.rmtree(cache_dir)
            print("âœ… ç¼“å­˜å·²æ¸…ç†")
        else:
            print("âŒ å–æ¶ˆæ“ä½œ")
    else:
        print("âœ… ç¼“å­˜ç›®å½•ä¸å­˜åœ¨")

def switch_mode():
    """åˆ‡æ¢è¿è¡Œæ¨¡å¼"""
    print("ğŸ”„ åˆ‡æ¢è¿è¡Œæ¨¡å¼")
    print("-" * 50)
    
    config_path = os.path.join(os.path.dirname(__file__), "config.json")
    if not os.path.exists(config_path):
        print("âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    with open(config_path, 'r') as f:
        config = json.load(f)
    
    current_mode = config.get('pytorch_version', 'cpu')
    print(f"å½“å‰æ¨¡å¼: {current_mode}")
    print()
    print("å¯ç”¨æ¨¡å¼:")
    print("1. CPUæ¨¡å¼ - å…¼å®¹æ€§å¥½ï¼Œé€Ÿåº¦æ…¢")
    print("2. GPUæ¨¡å¼ - éœ€è¦NVIDIA GPUå’ŒCUDA")
    print("3. åŒæ¨¡å¼ - åŒæ—¶æ”¯æŒCPUå’ŒGPU")
    
    choice = input("\né€‰æ‹©æ¨¡å¼ [1/2/3] (æŒ‰Enterå–æ¶ˆ): ").strip()
    
    if choice == "1":
        new_mode = "cpu"
    elif choice == "2":
        new_mode = "gpu"
    elif choice == "3":
        new_mode = "both"
    else:
        print("âŒ å–æ¶ˆæ“ä½œ")
        return
    
    if new_mode != current_mode:
        config['pytorch_version'] = new_mode
        with open(config_path, 'w') as f:
            json.dump(config, f, indent=2)
        print(f"âœ… å·²åˆ‡æ¢åˆ° {new_mode} æ¨¡å¼")
        print("ğŸ’¡ æç¤º: éœ€è¦é‡æ–°å®‰è£…PyTorchæ‰èƒ½ä½¿æ›´æ”¹ç”Ÿæ•ˆ")
    else:
        print("â„¹ æ¨¡å¼æœªæ”¹å˜")

if __name__ == "__main__":
    if len(sys.argv) > 1:
        if sys.argv[1] == "check":
            check_environment()
        elif sys.argv[1] == "clear":
            clear_cache()
        elif sys.argv[1] == "switch":
            switch_mode()
        else:
            print("ç”¨æ³•: python tools.py [check|clear|switch]")
            print("  check  - æ£€æŸ¥ç¯å¢ƒçŠ¶æ€")
            print("  clear  - æ¸…ç†æ¨¡å‹ç¼“å­˜")
            print("  switch - åˆ‡æ¢è¿è¡Œæ¨¡å¼")
    else:
        check_environment()
EOF
