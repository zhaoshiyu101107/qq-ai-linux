#!/bin/bash
# è‡ªåŒ…å«AIçŽ¯å¢ƒå®‰è£…è„šæœ¬ - æ”¯æŒGPUæ£€æµ‹å’Œæ¨¡åž‹é€‰æ‹©

set -e

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
MAGENTA='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m'

echo -e "${GREEN}ðŸš€ å¼€å§‹åˆ›å»ºè‡ªåŒ…å«AIçŽ¯å¢ƒ...${NC}"

# èŽ·å–è„šæœ¬æ‰€åœ¨ç›®å½•
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
AI_HOME="${SCRIPT_DIR}/ai_env"

echo "å®‰è£…ç›®å½•: ${AI_HOME}"

# åˆ›å»ºç›®å½•ç»“æž„
mkdir -p "${AI_HOME}"
cd "${AI_HOME}"

# åˆ›å»ºè™šæ‹ŸçŽ¯å¢ƒï¼ˆéš”ç¦»çŽ¯å¢ƒï¼‰
echo -e "\n${YELLOW}[1/7] åˆ›å»ºPythonè™šæ‹ŸçŽ¯å¢ƒ...${NC}"
python3 -m venv venv 2>/dev/null || python -m venv venv

# æ¿€æ´»è™šæ‹ŸçŽ¯å¢ƒ
source venv/bin/activate

# å‡çº§pip
echo -e "\n${YELLOW}[2/7] å‡çº§pip...${NC}"
pip install --upgrade pip > /dev/null 2>&1

# æ£€æµ‹GPU
echo -e "\n${YELLOW}[3/7] æ£€æµ‹ç³»ç»ŸçŽ¯å¢ƒ...${NC}"

# æ£€æµ‹NVIDIA GPU
HAS_NVIDIA=false
HAS_CUDA=false
CUDA_VERSION=""

# æ£€æŸ¥nvidia-smiå‘½ä»¤
if command -v nvidia-smi &> /dev/null; then
    HAS_NVIDIA=true
    echo -e "${GREEN}âœ“ æ£€æµ‹åˆ°NVIDIA GPU${NC}"
    
    # æ£€æŸ¥CUDAç‰ˆæœ¬
    if nvidia-smi --query | grep -q "CUDA Version"; then
        HAS_CUDA=true
        CUDA_VERSION=$(nvidia-smi --query | grep "CUDA Version" | awk '{print $NF}' | cut -d'.' -f1)
        echo -e "${GREEN}âœ“ æ£€æµ‹åˆ°CUDAç‰ˆæœ¬: ${CUDA_VERSION}${NC}"
    elif nvidia-smi | grep -q "CUDA Version"; then
        HAS_CUDA=true
        CUDA_VERSION=$(nvidia-smi | grep "CUDA Version" | awk '{print $NF}' | cut -d'.' -f1)
        echo -e "${GREEN}âœ“ æ£€æµ‹åˆ°CUDAç‰ˆæœ¬: ${CUDA_VERSION}${NC}"
    else
        echo -e "${YELLOW}âš  NVIDIAé©±åŠ¨å·²å®‰è£…ä½†æœªæ£€æµ‹åˆ°CUDAç‰ˆæœ¬${NC}"
        # å°è¯•ä»ŽnvccèŽ·å–CUDAç‰ˆæœ¬
        if command -v nvcc &> /dev/null; then
            CUDA_VERSION=$(nvcc --version | grep "release" | awk '{print $6}' | cut -c1-2)
            echo -e "${GREEN}âœ“ ä»Žnvccæ£€æµ‹åˆ°CUDAç‰ˆæœ¬: ${CUDA_VERSION}${NC}"
            HAS_CUDA=true
        fi
    fi
else
    echo -e "${YELLOW}â„¹ æœªæ£€æµ‹åˆ°NVIDIA GPUæˆ–é©±åŠ¨${NC}"
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–GPU
    if command -v lspci &> /dev/null; then
        if lspci | grep -i "vga\|3d\|display" | grep -qi "nvidia"; then
            echo -e "${YELLOW}âš  æ£€æµ‹åˆ°NVIDIAæ˜¾å¡ä½†é©±åŠ¨æœªå®‰è£…${NC}"
            HAS_NVIDIA=true
        fi
    fi
fi

# è¯¢é—®ç”¨æˆ·é€‰æ‹©PyTorchç‰ˆæœ¬
echo -e "\n${BLUE}è¯·é€‰æ‹©PyTorchå®‰è£…ç‰ˆæœ¬:${NC}"

if [ "$HAS_NVIDIA" = true ] && [ "$HAS_CUDA" = true ]; then
    echo "1. GPUåŠ é€Ÿç‰ˆ (CUDA ${CUDA_VERSION}.x) - æŽ¨èï¼Œéœ€è¦NVIDIA GPU"
    echo "2. CPUç‰ˆ - é€šç”¨å…¼å®¹ï¼Œæ— GPUåŠ é€Ÿ"
    echo "3. CPU+GPUç‰ˆ - åŒæ—¶å®‰è£…CPUå’ŒGPUæ”¯æŒ"
    echo "4. è‡ªåŠ¨é€‰æ‹© - æ ¹æ®ç³»ç»Ÿè‡ªåŠ¨é€‰æ‹©æœ€ä½³ç‰ˆæœ¬"
    
    read -p "è¯·é€‰æ‹© [1/2/3/4] (é»˜è®¤: 4): " choice
    
    case $choice in
        1)
            PYTORCH_VERSION="gpu"
            echo -e "${YELLOW}é€‰æ‹©: GPUåŠ é€Ÿç‰ˆæœ¬${NC}"
            ;;
        2)
            PYTORCH_VERSION="cpu"
            echo -e "${YELLOW}é€‰æ‹©: CPUç‰ˆæœ¬${NC}"
            ;;
        3)
            PYTORCH_VERSION="both"
            echo -e "${YELLOW}é€‰æ‹©: CPU+GPUç‰ˆæœ¬${NC}"
            ;;
        4|"")
            PYTORCH_VERSION="auto"
            echo -e "${YELLOW}é€‰æ‹©: è‡ªåŠ¨é€‰æ‹©ç‰ˆæœ¬${NC}"
            ;;
        *)
            PYTORCH_VERSION="auto"
            echo -e "${YELLOW}é€‰æ‹©: è‡ªåŠ¨é€‰æ‹©ç‰ˆæœ¬${NC}"
            ;;
    esac
elif [ "$HAS_NVIDIA" = true ] && [ "$HAS_CUDA" = false ]; then
    echo "1. CPUç‰ˆ - NVIDIAé©±åŠ¨å·²å®‰è£…ä½†CUDAå¯èƒ½ä¸å¯ç”¨"
    echo "2. å°è¯•å®‰è£…GPUç‰ˆ - å¯èƒ½éœ€è¦é¢å¤–é…ç½®CUDA"
    echo "3. è‡ªåŠ¨é€‰æ‹© - æ ¹æ®ç³»ç»Ÿè‡ªåŠ¨é€‰æ‹©"
    
    read -p "è¯·é€‰æ‹© [1/2/3] (é»˜è®¤: 3): " choice
    
    case $choice in
        1)
            PYTORCH_VERSION="cpu"
            echo -e "${YELLOW}é€‰æ‹©: CPUç‰ˆæœ¬${NC}"
            ;;
        2)
            PYTORCH_VERSION="gpu"
            echo -e "${YELLOW}é€‰æ‹©: å°è¯•å®‰è£…GPUç‰ˆæœ¬${NC}"
            echo -e "${YELLOW}æ³¨æ„: å¦‚æžœCUDAæœªæ­£ç¡®å®‰è£…ï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨å®‰è£…CUDAå·¥å…·åŒ…${NC}"
            ;;
        3|"")
            PYTORCH_VERSION="auto"
            echo -e "${YELLOW}é€‰æ‹©: è‡ªåŠ¨é€‰æ‹©ç‰ˆæœ¬${NC}"
            ;;
        *)
            PYTORCH_VERSION="auto"
            echo -e "${YELLOW}é€‰æ‹©: è‡ªåŠ¨é€‰æ‹©ç‰ˆæœ¬${NC}"
            ;;
    esac
else
    echo "1. CPUç‰ˆ - å”¯ä¸€å¯ç”¨é€‰é¡¹"
    echo "2. è‡ªåŠ¨é€‰æ‹© - æ ¹æ®ç³»ç»Ÿè‡ªåŠ¨é€‰æ‹©"
    read -p "è¯·é€‰æ‹© [1/2] (é»˜è®¤: 2): " choice
    
    if [ "$choice" = "1" ]; then
        PYTORCH_VERSION="cpu"
        echo -e "${YELLOW}é€‰æ‹©: CPUç‰ˆæœ¬${NC}"
    else
        PYTORCH_VERSION="auto"
        echo -e "${YELLOW}é€‰æ‹©: è‡ªåŠ¨é€‰æ‹©ç‰ˆæœ¬${NC}"
    fi
fi

# æ ¹æ®é€‰æ‹©å®‰è£…PyTorch
echo -e "\n${YELLOW}[4/7] å®‰è£…PyTorch...${NC}"

if [ "$PYTORCH_VERSION" = "auto" ]; then
    if [ "$HAS_NVIDIA" = true ] && [ "$HAS_CUDA" = true ]; then
        echo "è‡ªåŠ¨é€‰æ‹©: å®‰è£…GPUç‰ˆæœ¬ (CUDA ${CUDA_VERSION}.x)"
        PYTORCH_VERSION="gpu"
    else
        echo "è‡ªåŠ¨é€‰æ‹©: å®‰è£…CPUç‰ˆæœ¬"
        PYTORCH_VERSION="cpu"
    fi
fi

case $PYTORCH_VERSION in
    "cpu")
        echo "å®‰è£…PyTorch CPUç‰ˆæœ¬..."
        pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu > /dev/null 2>&1
        ;;
    "gpu")
        echo "å®‰è£…PyTorch GPUç‰ˆæœ¬..."
        
        # æ ¹æ®æ£€æµ‹åˆ°çš„CUDAç‰ˆæœ¬é€‰æ‹©å¯¹åº”çš„PyTorch
        if [ -n "$CUDA_VERSION" ]; then
            if [ "$CUDA_VERSION" = "12" ] || [ "$CUDA_VERSION" -ge 12 ]; then
                echo "ä½¿ç”¨CUDA 12.1ç‰ˆæœ¬..."
                pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 > /dev/null 2>&1
            elif [ "$CUDA_VERSION" = "11" ]; then
                echo "ä½¿ç”¨CUDA 11.8ç‰ˆæœ¬..."
                pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 > /dev/null 2>&1
            else
                echo "ä½¿ç”¨é»˜è®¤CUDAç‰ˆæœ¬ (11.8)..."
                pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 > /dev/null 2>&1
            fi
        else
            echo "ä½¿ç”¨é»˜è®¤CUDAç‰ˆæœ¬ (11.8)..."
            pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 > /dev/null 2>&1
        fi
        ;;
    "both")
        echo "å®‰è£…PyTorch CPU+GPUç‰ˆæœ¬..."
        echo "æ³¨æ„: è¿™å°†å®‰è£…è¾ƒå¤§çš„åŒ…ï¼ŒåŒæ—¶æ”¯æŒCPUå’ŒGPUè¿è¡Œ"
        pip install torch torchvision torchaudio > /dev/null 2>&1
        ;;
esac

# å®‰è£…Transformerså’Œå…¶ä»–AIåº“
echo -e "\n${YELLOW}[5/7] å®‰è£…Transformerså’Œå…¶ä»–AIåº“...${NC}"
echo "å®‰è£…Transformers..."
pip install transformers accelerate sentencepiece protobuf einops tiktoken > /dev/null 2>&1

# è¯¢é—®ç”¨æˆ·é€‰æ‹©æ¨¡åž‹
echo -e "\n${YELLOW}[6/7] é€‰æ‹©AIæ¨¡åž‹...${NC}"
echo -e "${BLUE}è¯·é€‰æ‹©è¦ä½¿ç”¨çš„AIæ¨¡åž‹:${NC}"
echo "1. Qwen/Qwen3-0.5B-Instruct (è½»é‡çº§, çº¦0.5GB)"
echo "2. Qwen/Qwen3-0.6B-Instruct (æŽ¨è, çº¦1.2GB)"
echo "3. Qwen/Qwen3-1.8B-Instruct (å¹³è¡¡, çº¦3.6GB)"
echo "4. Qwen/Qwen3-4B-Instruct (æ€§èƒ½å¥½, çº¦8GB)"
echo "5. Qwen/Qwen2.5-0.5B-Instruct (æ–°ç‰ˆ, çº¦0.5GB)"
echo "6. microsoft/phi-2 (å¾®è½¯Phi-2, çº¦2.7GB)"
echo "7. TinyLlama/TinyLlama-1.1B-Chat-v1.0 (å°ç¾Šé©¼, çº¦2.2GB)"
echo "8. è‡ªå®šä¹‰æ¨¡åž‹ (è¾“å…¥å®Œæ•´çš„HuggingFaceæ¨¡åž‹è·¯å¾„)"

read -p "è¯·é€‰æ‹© [1-8] (é»˜è®¤: 2): " model_choice

case $model_choice in
    1)
        MODEL_NAME="Qwen/Qwen3-0.5B-Instruct"
        MODEL_SIZE="çº¦0.5GB"
        ;;
    3)
        MODEL_NAME="Qwen/Qwen3-1.8B-Instruct"
        MODEL_SIZE="çº¦3.6GB"
        ;;
    4)
        MODEL_NAME="Qwen/Qwen3-4B-Instruct"
        MODEL_SIZE="çº¦8GB"
        ;;
    5)
        MODEL_NAME="Qwen/Qwen2.5-0.5B-Instruct"
        MODEL_SIZE="çº¦0.5GB"
        ;;
    6)
        MODEL_NAME="microsoft/phi-2"
        MODEL_SIZE="çº¦2.7GB"
        ;;
    7)
        MODEL_NAME="TinyLlama/TinyLlama-1.1B-Chat-v1.0"
        MODEL_SIZE="çº¦2.2GB"
        ;;
    8)
        read -p "è¯·è¾“å…¥å®Œæ•´çš„HuggingFaceæ¨¡åž‹è·¯å¾„ (ä¾‹å¦‚: Qwen/Qwen3-0.6B-Instruct): " custom_model
        if [ -n "$custom_model" ]; then
            MODEL_NAME="$custom_model"
            MODEL_SIZE="æœªçŸ¥å¤§å°"
            echo -e "${YELLOW}æ³¨æ„: ä½¿ç”¨è‡ªå®šä¹‰æ¨¡åž‹: ${MODEL_NAME}${NC}"
        else
            MODEL_NAME="Qwen/Qwen3-0.6B-Instruct"
            MODEL_SIZE="çº¦1.2GB"
            echo -e "${YELLOW}ä½¿ç”¨é»˜è®¤æ¨¡åž‹: ${MODEL_NAME}${NC}"
        fi
        ;;
    2|"")
        MODEL_NAME="Qwen/Qwen3-0.6B-Instruct"
        MODEL_SIZE="çº¦1.2GB"
        ;;
    *)
        MODEL_NAME="Qwen/Qwen3-0.6B-Instruct"
        MODEL_SIZE="çº¦1.2GB"
        echo -e "${YELLOW}ä½¿ç”¨é»˜è®¤æ¨¡åž‹: ${MODEL_NAME}${NC}"
        ;;
esac

# è¯¢é—®æ˜¯å¦å®‰è£…é™„åŠ ç»„ä»¶
echo -e "\n${BLUE}æ˜¯å¦å®‰è£…é¢å¤–çš„AIç»„ä»¶ï¼Ÿ${NC}"
echo "1. åŸºç¡€ç»„ä»¶ (å·²å®‰è£…)"
echo "2. æ·»åŠ LangChainæ”¯æŒ (AIåº”ç”¨å¼€å‘)"
echo "3. æ·»åŠ Gradio Webç•Œé¢"
echo "4. æ·»åŠ Jupyteræ”¯æŒ (äº¤äº’å¼ç¼–ç¨‹)"
echo "5. å…¨éƒ¨å®‰è£…"

read -p "è¯·é€‰æ‹© [1-5] (é»˜è®¤: 1): " extra_choice

case $extra_choice in
    2)
        echo "å®‰è£…LangChain..."
        pip install langchain langchain-community > /dev/null 2>&1
        EXTRA_PACKAGES="langchain"
        ;;
    3)
        echo "å®‰è£…Gradio..."
        pip install gradio > /dev/null 2>&1
        EXTRA_PACKAGES="gradio"
        ;;
    4)
        echo "å®‰è£…Jupyter..."
        pip install jupyter ipykernel > /dev/null 2>&1
        EXTRA_PACKAGES="jupyter"
        ;;
    5)
        echo "å®‰è£…æ‰€æœ‰é¢å¤–ç»„ä»¶..."
        pip install langchain langchain-community gradio jupyter ipykernel > /dev/null 2>&1
        EXTRA_PACKAGES="all"
        ;;
    *)
        echo "ä»…å®‰è£…åŸºç¡€ç»„ä»¶"
        EXTRA_PACKAGES="none"
        ;;
esac

# åˆ›å»ºé…ç½®æ–‡ä»¶
echo -e "\n${YELLOW}[7/7] åˆ›å»ºé…ç½®æ–‡ä»¶...${NC}"

# èŽ·å–å½“å‰æ—¥æœŸ
CURRENT_DATE=$(date '+%Y-%m-%d %H:%M:%S')

# åˆ›å»ºæ¨¡åž‹é…ç½®ç›®å½•
mkdir -p model_configs

cat > config.json << EOF
{
  "environment": "local",
  "model": "${MODEL_NAME}",
  "model_size": "${MODEL_SIZE}",
  "install_date": "${CURRENT_DATE}",
  "install_dir": "${AI_HOME}",
  "pytorch_version": "${PYTORCH_VERSION}",
  "has_gpu": ${HAS_NVIDIA},
  "has_cuda": ${HAS_CUDA},
  "cuda_version": "${CUDA_VERSION:-null}",
  "extra_packages": "${EXTRA_PACKAGES}",
  "requirements": [
    "torch",
    "transformers",
    "accelerate",
    "sentencepiece",
    "protobuf",
    "einops",
    "tiktoken"
  ]
}
EOF

# ä¸ºé€‰æ‹©çš„æ¨¡åž‹åˆ›å»ºç‰¹å®šé…ç½®
MODEL_SHORT_NAME=$(echo "$MODEL_NAME" | sed 's/[\/-]/_/g')
cat > "model_configs/${MODEL_SHORT_NAME}.json" << EOF
{
  "model_name": "${MODEL_NAME}",
  "model_type": "causal_lm",
  "tokenizer_class": "AutoTokenizer",
  "model_class": "AutoModelForCausalLM",
  "trust_remote_code": true,
  "features": {
    "chat_template": true,
    "generation": true,
    "streaming": false
  },
  "recommended_settings": {
    "max_length": 4096,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
EOF

# åˆ›å»ºå¯åŠ¨è„šæœ¬
echo "åˆ›å»ºå¯åŠ¨è„šæœ¬..."

cat > start_ai.sh << EOF
#!/bin/bash
# AIæ¨¡åž‹å¯åŠ¨è„šæœ¬

SCRIPT_DIR="\$(cd "\$(dirname "\${BASH_SOURCE[0]}")" && pwd)"
cd "\$SCRIPT_DIR"

echo "=========================================="
echo "ðŸ¤– æœ¬åœ°AIè¿è¡ŒçŽ¯å¢ƒ"
echo "=========================================="
echo "æ¨¡åž‹: ${MODEL_NAME}"
echo "ä½ç½®: \$SCRIPT_DIR"
echo ""

# æ£€æŸ¥æ˜¯å¦åœ¨è™šæ‹ŸçŽ¯å¢ƒä¸­
if [ -z "\$VIRTUAL_ENV" ]; then
    echo "æ¿€æ´»è™šæ‹ŸçŽ¯å¢ƒ..."
    if [ -f "venv/bin/activate" ]; then
        source venv/bin/activate
    else
        echo "é”™è¯¯: æœªæ‰¾åˆ°è™šæ‹ŸçŽ¯å¢ƒ"
        exit 1
    fi
fi

# è¿è¡ŒPythonè„šæœ¬
python run_ai.py "\$@"
EOF

chmod +x start_ai.sh

# åˆ›å»ºæ¨¡åž‹é€‰æ‹©è„šæœ¬
cat > switch_model.sh << 'EOF'
#!/bin/bash
# åˆ‡æ¢AIæ¨¡åž‹è„šæœ¬

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
cd "$SCRIPT_DIR"

if [ -z "$VIRTUAL_ENV" ]; then
    if [ -f "venv/bin/activate" ]; then
        source venv/bin/activate
    fi
fi

python switch_model.py "$@"
EOF

chmod +x switch_model.sh

# åˆ›å»ºä¸»è¿è¡Œè„šæœ¬
cat > run_ai.py << 'EOF'
#!/usr/bin/env python3
"""
AIæ¨¡åž‹æœ¬åœ°è¿è¡Œè„šæœ¬
æ”¯æŒå¤šç§æ¨¡åž‹ï¼Œå®Œå…¨æœ¬åœ°è¿è¡Œ
"""

import os
import sys
import torch
import json
import argparse
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = os.path.join(os.path.dirname(__file__), "config.json")
    if os.path.exists(config_path):
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}

def check_gpu_info():
    """æ£€æŸ¥GPUä¿¡æ¯"""
    config = load_config()
    
    print("=" * 50)
    print("ðŸ¤– æœ¬åœ°AIçŽ¯å¢ƒ")
    print("=" * 50)
    print(f"ç›®å½•: {os.path.dirname(os.path.abspath(__file__))}")
    print(f"æ¨¡åž‹: {config.get('model', 'æœªçŸ¥')}")
    print(f"æ¨¡åž‹å¤§å°: {config.get('model_size', 'æœªçŸ¥')}")
    print(f"Python: {sys.version.split()[0]}")
    print(f"PyTorch: {torch.__version__}")
    
    # æ£€æŸ¥CUDA
    cuda_available = torch.cuda.is_available()
    print(f"CUDAå¯ç”¨: {cuda_available}")
    
    if cuda_available:
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
            print(f"    å†…å­˜: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB")
    else:
        print("âš  ä½¿ç”¨CPUæ¨¡å¼ - é€Ÿåº¦è¾ƒæ…¢")
        if config.get('has_gpu'):
            print("ðŸ’¡ æç¤º: æ£€æµ‹åˆ°GPUä½†PyTorchæ— æ³•è®¿é—®")
            print("ðŸ’¡ å¯èƒ½åŽŸå› : PyTorchå®‰è£…çš„æ˜¯CPUç‰ˆæœ¬")
            print("ðŸ’¡ è§£å†³æ–¹æ¡ˆ: é‡æ–°å®‰è£…GPUç‰ˆæœ¬çš„PyTorch")
    
    print("=" * 50)
    print()

def load_model():
    """åŠ è½½AIæ¨¡åž‹"""
    from transformers import AutoTokenizer, AutoModelForCausalLM
    
    config = load_config()
    model_name = config.get('model', 'Qwen/Qwen3-0.6B-Instruct')
    model_size = config.get('model_size', 'çº¦1.2GB')
    
    print(f"æ­£åœ¨åŠ è½½æ¨¡åž‹: {model_name}")
    print(f"æ¨¡åž‹å¤§å°: {model_size}")
    print("é¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½æ¨¡åž‹æ–‡ä»¶")
    print("ä¸‹è½½å®ŒæˆåŽä¼šç¼“å­˜ï¼Œä¸‹æ¬¡æ— éœ€é‡æ–°ä¸‹è½½")
    print()
    
    # è®¾ç½®æ¨¡åž‹ç¼“å­˜ç›®å½•åˆ°æœ¬åœ°
    cache_dir = os.path.join(os.path.dirname(__file__), "model_cache")
    os.makedirs(cache_dir, exist_ok=True)
    
    try:
        # åŠ è½½tokenizer
        print("1. åŠ è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            trust_remote_code=True
        )
        
        # åŠ è½½æ¨¡åž‹
        print("2. åŠ è½½æ¨¡åž‹...")
        
        # æ ¹æ®æ˜¯å¦æœ‰GPUé€‰æ‹©åŠ è½½æ–¹å¼
        if torch.cuda.is_available():
            print("  ä½¿ç”¨GPUåŠ é€Ÿ")
            try:
                # å°è¯•ä½¿ç”¨GPU
                model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype=torch.float16,
                    device_map="auto",
                    cache_dir=cache_dir,
                    trust_remote_code=True
                )
            except Exception as e:
                print(f"  GPUåŠ è½½å¤±è´¥: {e}")
                print("  å›žé€€åˆ°CPUæ¨¡å¼...")
                model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype=torch.float32,
                    device_map="cpu",
                    cache_dir=cache_dir,
                    trust_remote_code=True
                )
        else:
            print("  ä½¿ç”¨CPUè¿è¡Œ")
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float32,
                device_map="cpu",
                cache_dir=cache_dir,
                trust_remote_code=True
            )
        
        return tokenizer, model
        
    except Exception as e:
        print(f"\nâŒ åŠ è½½æ¨¡åž‹å¤±è´¥: {e}")
        return None, None

def chat_loop(tokenizer, model):
    """å¯¹è¯å¾ªçŽ¯"""
    print()
    print("âœ… æ¨¡åž‹åŠ è½½æˆåŠŸï¼")
    print("-" * 50)
    print("ðŸ’¡ æç¤º: è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
    print("-" * 50)
    
    while True:
        try:
            user_input = input("\nä½ : ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'q']:
                print("å†è§ï¼ðŸ‘‹")
                break
                
            if not user_input:
                continue
            
            # å‡†å¤‡å¯¹è¯æ ¼å¼
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¹äºŽåŠ©äººçš„AIåŠ©æ‰‹ã€‚"},
                {"role": "user", "content": user_input}
            ]
            
            try:
                text = tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
            except:
                # å¦‚æžœæ¨¡åž‹ä¸æ”¯æŒchat_templateï¼Œä½¿ç”¨ç®€å•æ ¼å¼
                text = f"ç”¨æˆ·: {user_input}\nåŠ©æ‰‹:"
            
            # ç¼–ç å¹¶ç”Ÿæˆ
            model_inputs = tokenizer(text, return_tensors="pt").to(model.device)
            
            print("æ€è€ƒä¸­...", end="", flush=True)
            
            with torch.no_grad():
                outputs = model.generate(
                    **model_inputs,
                    max_new_tokens=200,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            print("\r" + " " * 20, end="\r")  # æ¸…é™¤"æ€è€ƒä¸­..."
            
            # è§£ç å›žå¤
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # æå–æ¨¡åž‹å›žå¤éƒ¨åˆ†
            if "åŠ©æ‰‹:" in response:
                response = response.split("åŠ©æ‰‹:")[-1].strip()
            elif "assistant" in response:
                response = response.split("assistant")[-1].strip()
            elif "Assistant:" in response:
                response = response.split("Assistant:")[-1].strip()
            
            print(f"AI: {response}")
            
        except KeyboardInterrupt:
            print("\n\né€€å‡ºç¨‹åº")
            break
        except Exception as e:
            print(f"\né”™è¯¯: {e}")
            continue

def main():
    parser = argparse.ArgumentParser(description="è¿è¡Œæœ¬åœ°AIæ¨¡åž‹")
    parser.add_argument("--model", type=str, help="æŒ‡å®šè¦ä½¿ç”¨çš„æ¨¡åž‹")
    parser.add_argument("--check", action="store_true", help="ä»…æ£€æŸ¥çŽ¯å¢ƒ")
    parser.add_argument("--web", action="store_true", help="å¯åŠ¨Webç•Œé¢")
    parser.add_argument("--api", action="store_true", help="å¯åŠ¨APIæœåŠ¡")
    
    args = parser.parse_args()
    
    if args.check:
        check_gpu_info()
        return
    
    check_gpu_info()
    
    if args.model:
        # åˆ‡æ¢åˆ°æŒ‡å®šæ¨¡åž‹
        config = load_config()
        config['model'] = args.model
        config_path = os.path.join(os.path.dirname(__file__), "config.json")
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2)
        print(f"å·²åˆ‡æ¢åˆ°æ¨¡åž‹: {args.model}")
    
    # åŠ è½½æ¨¡åž‹
    tokenizer, model = load_model()
    
    if tokenizer is None or model is None:
        print("\nðŸ’¡ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
        print("1. æ£€æŸ¥ç½‘ç»œè¿žæŽ¥")
        print("2. ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´")
        print("3. æ£€æŸ¥æ¨¡åž‹åç§°æ˜¯å¦æ­£ç¡®")
        print(f"4. å°è¯•å…¶ä»–æ¨¡åž‹: python switch_model.py")
        return
    
    if args.web:
        # å¯åŠ¨Webç•Œé¢
        try:
            import gradio as gr
            print("å¯åŠ¨Webç•Œé¢...")
            
            def respond(message, history):
                inputs = tokenizer(message, return_tensors="pt").to(model.device)
                with torch.no_grad():
                    outputs = model.generate(**inputs, max_new_tokens=200)
                response = tokenizer.decode(outputs[0], skip_special_tokens=True)
                return response
            
            gr.ChatInterface(respond).launch(server_name="0.0.0.0", server_port=7860)
        except ImportError:
            print("æœªå®‰è£…gradioï¼Œæ— æ³•å¯åŠ¨Webç•Œé¢")
            print("è¯·è¿è¡Œ: pip install gradio")
            chat_loop(tokenizer, model)
    elif args.api:
        # å¯åŠ¨APIæœåŠ¡
        try:
            from fastapi import FastAPI
            import uvicorn
            print("å¯åŠ¨APIæœåŠ¡...")
            
            app = FastAPI()
            
            @app.post("/chat")
            async def chat_endpoint(message: dict):
                user_input = message.get("message", "")
                inputs = tokenizer(user_input, return_tensors="pt").to(model.device)
                with torch.no_grad():
                    outputs = model.generate(**inputs, max_new_tokens=200)
                response = tokenizer.decode(outputs[0], skip_special_tokens=True)
                return {"response": response}
            
            uvicorn.run(app, host="0.0.0.0", port=8000)
        except ImportError:
            print("æœªå®‰è£…fastapiå’Œuvicornï¼Œæ— æ³•å¯åŠ¨APIæœåŠ¡")
            print("è¯·è¿è¡Œ: pip install fastapi uvicorn")
            chat_loop(tokenizer, model)
    else:
        # å¯åŠ¨äº¤äº’å¼èŠå¤©
        chat_loop(tokenizer, model)

if __name__ == "__main__":
    main()
EOF

# åˆ›å»ºæ¨¡åž‹åˆ‡æ¢è„šæœ¬
cat > switch_model.py << 'EOF'
#!/usr/bin/env python3
"""
AIæ¨¡åž‹åˆ‡æ¢è„šæœ¬
"""

import os
import sys
import json

def main():
    print("ðŸ¤– AIæ¨¡åž‹åˆ‡æ¢å·¥å…·")
    print("=" * 50)
    
    config_path = os.path.join(os.path.dirname(__file__), "config.json")
    if not os.path.exists(config_path):
        print("âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    current_model = config.get('model', 'æœªçŸ¥')
    print(f"å½“å‰æ¨¡åž‹: {current_model}")
    print()
    
    # å¯ç”¨æ¨¡åž‹åˆ—è¡¨
    available_models = {
        "1": ("Qwen/Qwen3-0.5B-Instruct", "è½»é‡çº§, çº¦0.5GB"),
        "2": ("Qwen/Qwen3-0.6B-Instruct", "æŽ¨è, çº¦1.2GB"),
        "3": ("Qwen/Qwen3-1.8B-Instruct", "å¹³è¡¡, çº¦3.6GB"),
        "4": ("Qwen/Qwen3-4B-Instruct", "æ€§èƒ½å¥½, çº¦8GB"),
        "5": ("Qwen/Qwen2.5-0.5B-Instruct", "æ–°ç‰ˆ, çº¦0.5GB"),
        "6": ("microsoft/phi-2", "å¾®è½¯Phi-2, çº¦2.7GB"),
        "7": ("TinyLlama/TinyLlama-1.1B-Chat-v1.0", "å°ç¾Šé©¼, çº¦2.2GB"),
        "8": ("è‡ªå®šä¹‰", "è¾“å…¥å®Œæ•´çš„HuggingFaceæ¨¡åž‹è·¯å¾„")
    }
    
    print("å¯ç”¨æ¨¡åž‹:")
    for key, (name, desc) in available_models.items():
        print(f"{key}. {name} - {desc}")
    
    print()
    choice = input("é€‰æ‹©æ¨¡åž‹ç¼–å· [1-8] (æŒ‰Enterå–æ¶ˆ): ").strip()
    
    if not choice:
        print("âŒ å–æ¶ˆæ“ä½œ")
        return
    
    if choice == "8":
        custom_model = input("è¯·è¾“å…¥å®Œæ•´çš„HuggingFaceæ¨¡åž‹è·¯å¾„: ").strip()
        if custom_model:
            new_model = custom_model
            model_size = "æœªçŸ¥å¤§å°"
        else:
            print("âŒ æœªè¾“å…¥æ¨¡åž‹è·¯å¾„")
            return
    elif choice in available_models:
        new_model, model_desc = available_models[choice]
        model_size = model_desc.split(",")[-1].strip()
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©")
        return
    
    if new_model == current_model:
        print(f"â„¹ æ¨¡åž‹æœªæ”¹å˜: {current_model}")
        return
    
    # æ›´æ–°é…ç½®
    config['model'] = new_model
    config['model_size'] = model_size
    
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2)
    
    print(f"âœ… å·²åˆ‡æ¢åˆ°æ¨¡åž‹: {new_model}")
    print(f"ðŸ“Š æ¨¡åž‹å¤§å°: {model_size}")
    print()
    print("ðŸ’¡ ä¸‹æ¬¡å¯åŠ¨æ—¶å°†ä½¿ç”¨æ–°æ¨¡åž‹")
    print("ðŸ’¡ é¦–æ¬¡ä½¿ç”¨æ–°æ¨¡åž‹éœ€è¦ä¸‹è½½æ¨¡åž‹æ–‡ä»¶")

if __name__ == "__main__":
    main()
EOF

# åˆ›å»ºå·¥å…·è„šæœ¬
cat > tools.py << 'EOF'
#!/usr/bin/env python3
"""
AIçŽ¯å¢ƒå·¥å…·è„šæœ¬
"""

import torch
import sys
import os
import json

def check_environment():
    """æ£€æŸ¥çŽ¯å¢ƒçŠ¶æ€"""
    print("ðŸ” çŽ¯å¢ƒæ£€æŸ¥")
    print("=" * 50)
    
    # è¯»å–é…ç½®æ–‡ä»¶
    config_path = os.path.join(os.path.dirname(__file__), "config.json")
    if os.path.exists(config_path):
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        print(f"å®‰è£…æ—¥æœŸ: {config.get('install_date', 'æœªçŸ¥')}")
        print(f"å½“å‰æ¨¡åž‹: {config.get('model', 'æœªçŸ¥')}")
        print(f"æ¨¡åž‹å¤§å°: {config.get('model_size', 'æœªçŸ¥')}")
        print(f"PyTorchç‰ˆæœ¬: {config.get('pytorch_version', 'æœªçŸ¥')}")
        print(f"æ£€æµ‹åˆ°GPU: {config.get('has_gpu', False)}")
        if config.get('has_cuda'):
            print(f"CUDAç‰ˆæœ¬: {config.get('cuda_version', 'æœªçŸ¥')}")
    else:
        print("é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°")
    
    print("=" * 50)
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
            print(f"    å†…å­˜: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB")
    else:
        print("âš  å½“å‰è¿è¡Œåœ¨CPUæ¨¡å¼")
        if config.get('has_gpu'):
            print("ðŸ’¡ æ£€æµ‹åˆ°GPUä½†PyTorchæ— æ³•è®¿é—®")
            print("ðŸ’¡ å¯èƒ½å®‰è£…äº†CPUç‰ˆæœ¬çš„PyTorch")
            print("ðŸ’¡ å»ºè®®é‡æ–°å®‰è£…GPUç‰ˆæœ¬: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
    
    # æ£€æŸ¥å…¶ä»–åŒ…
    packages = [
        ("transformers", "transformers"),
        ("accelerate", "accelerate"),
        ("sentencepiece", "sentencepiece"),
        ("einops", "einops"),
    ]
    
    print("=" * 50)
    for name, module in packages:
        try:
            __import__(module)
            print(f"âœ… {name}: å·²å®‰è£…")
        except ImportError:
            print(f"âŒ {name}: æœªå®‰è£…")
    
    print("=" * 50)

def clear_cache():
    """æ¸…ç†æ¨¡åž‹ç¼“å­˜"""
    import shutil
    cache_dir = os.path.join(os.path.dirname(__file__), "model_cache")
    
    if os.path.exists(cache_dir):
        total_size = 0
        file_count = 0
        
        for root, dirs, files in os.walk(cache_dir):
            for file in files:
                file_path = os.path.join(root, file)
                total_size += os.path.getsize(file_path)
                file_count += 1
        
        size_gb = total_size / (1024**3)
        
        print(f"ç¼“å­˜æ–‡ä»¶: {file_count} ä¸ª")
        print(f"ç¼“å­˜å¤§å°: {size_gb:.2f} GB")
        print()
        
        response = input("ç¡®è®¤åˆ é™¤æ‰€æœ‰ç¼“å­˜æ–‡ä»¶ï¼Ÿ(y/N): ").strip().lower()
        
        if response == 'y':
            shutil.rmtree(cache_dir)
            print("âœ… ç¼“å­˜å·²æ¸…ç†")
        else:
            print("âŒ å–æ¶ˆæ“ä½œ")
    else:
        print("âœ… ç¼“å­˜ç›®å½•ä¸å­˜åœ¨")

def diagnose_gpu():
    """è¯Šæ–­GPUé—®é¢˜"""
    print("ðŸ”§ GPUè¯Šæ–­å·¥å…·")
    print("=" * 50)
    
    print("PyTorchä¿¡æ¯:")
    print(f"  ç‰ˆæœ¬: {torch.__version__}")
    print(f"  CUDAç¼–è¯‘ç‰ˆæœ¬: {torch.version.cuda}")
    print(f"  torch.cuda.is_available(): {torch.cuda.is_available()}")
    
    print("\nç³»ç»Ÿä¿¡æ¯:")
    import platform
    print(f"  æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.release()}")
    
    # æ£€æŸ¥nvidia-smi
    print("\næ£€æŸ¥NVIDIAé©±åŠ¨:")
    import subprocess
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        if result.returncode == 0:
            print("âœ… nvidia-smiå‘½ä»¤å¯ç”¨")
            lines = result.stdout.split('\n')
            for line in lines[:5]:
                if line.strip():
                    print(f"  {line}")
        else:
            print("âŒ nvidia-smiå‘½ä»¤ä¸å¯ç”¨")
    except FileNotFoundError:
        print("âŒ æœªæ‰¾åˆ°nvidia-smiå‘½ä»¤")
    
    print("\nçŽ¯å¢ƒå˜é‡æ£€æŸ¥:")
    env_vars = ['CUDA_HOME', 'CUDA_PATH', 'PATH']
    for var in env_vars:
        value = os.environ.get(var, 'æœªè®¾ç½®')
        if var == 'PATH':
            print(f"  {var}: (é•¿åº¦: {len(value)} å­—ç¬¦)")
        else:
            print(f"  {var}: {value}")
    
    print("\nðŸ’¡ å»ºè®®:")
    if not torch.cuda.is_available():
        print("1. ç¡®ä¿å·²å®‰è£…NVIDIAé©±åŠ¨")
        print("2. å®‰è£…CUDAå·¥å…·åŒ…")
        print("3. é‡æ–°å®‰è£…GPUç‰ˆæœ¬çš„PyTorch:")
        print("   pip uninstall torch torchvision torchaudio")
        print("   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")

if __name__ == "__main__":
    if len(sys.argv) > 1:
        if sys.argv[1] == "check":
            check_environment()
        elif sys.argv[1] == "clear":
            clear_cache()
        elif sys.argv[1] == "diagnose":
            diagnose_gpu()
        else:
            print("ç”¨æ³•: python tools.py [check|clear|diagnose]")
            print("  check    - æ£€æŸ¥çŽ¯å¢ƒçŠ¶æ€")
            print("  clear    - æ¸…ç†æ¨¡åž‹ç¼“å­˜")
            print("  diagnose - è¯Šæ–­GPUé—®é¢˜")
    else:
        check_environment()
EOF

# åˆ›å»ºä¿®å¤è„šæœ¬ä¸“é—¨é’ˆå¯¹GPUé—®é¢˜
cat > fix_gpu.sh << 'EOF'
#!/bin/bash
# ä¿®å¤GPUæ”¯æŒè„šæœ¬

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
cd "$SCRIPT_DIR"

echo "ðŸ”§ ä¿®å¤GPUæ”¯æŒ"
echo "================"

if [ -z "$VIRTUAL_ENV" ]; then
    if [ -f "venv/bin/activate" ]; then
        source venv/bin/activate
    else
        echo "âŒ æœªæ‰¾åˆ°è™šæ‹ŸçŽ¯å¢ƒ"
        exit 1
    fi
fi

# æ£€æŸ¥å½“å‰PyTorchç‰ˆæœ¬
echo "å½“å‰PyTorchç‰ˆæœ¬:"
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDAå¯ç”¨: {torch.cuda.is_available()}')"

echo -e "\næ˜¯å¦é‡æ–°å®‰è£…GPUç‰ˆæœ¬çš„PyTorchï¼Ÿ (y/N)"
read -p "é€‰æ‹©: " choice

if [ "$choice" = "y" ] || [ "$choice" = "Y" ]; then
    echo "æ­£åœ¨å¸è½½å½“å‰PyTorch..."
    pip uninstall torch torchvision torchaudio -y
    
    echo -e "\né€‰æ‹©CUDAç‰ˆæœ¬:"
    echo "1. CUDA 11.8 (å…¼å®¹æ€§å¥½)"
    echo "2. CUDA 12.1 (è¾ƒæ–°ç‰ˆæœ¬)"
    read -p "é€‰æ‹© [1/2] (é»˜è®¤: 1): " cuda_choice
    
    if [ "$cuda_choice" = "2" ]; then
        echo "å®‰è£…CUDA 12.1ç‰ˆæœ¬..."
        pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
    else
        echo "å®‰è£…CUDA 11.8ç‰ˆæœ¬..."
        pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
    fi
    
    echo -e "\nâœ… å®‰è£…å®Œæˆï¼"
    echo "æ–°çš„PyTorchç‰ˆæœ¬:"
    python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDAå¯ç”¨: {torch.cuda.is_available()}')"
else
    echo "âŒ å–æ¶ˆæ“ä½œ"
fi
EOF

chmod +x fix_gpu.sh

# åˆ›å»ºREADMEæ–‡ä»¶
cat > README.md << EOF
# ðŸ¤– æœ¬åœ°AIçŽ¯å¢ƒ

è¿™æ˜¯ä¸€ä¸ªå®Œå…¨è‡ªåŒ…å«çš„AIè¿è¡ŒçŽ¯å¢ƒï¼Œæ”¯æŒå¤šç§AIæ¨¡åž‹å’ŒGPUåŠ é€Ÿã€‚

## ðŸ“¦ å®‰è£…ä¿¡æ¯

- **å®‰è£…ç›®å½•**: ${AI_HOME}
- **é€‰æ‹©æ¨¡åž‹**: ${MODEL_NAME} (${MODEL_SIZE})
- **PyTorchç‰ˆæœ¬**: ${PYTORCH_VERSION}
- **GPUæ”¯æŒ**: ${HAS_NVIDIA}
- **CUDAç‰ˆæœ¬**: ${CUDA_VERSION:-æœªæ£€æµ‹åˆ°}
- **å®‰è£…æ—¶é—´**: ${CURRENT_DATE}

## ðŸš€ å¿«é€Ÿå¼€å§‹

1. **å¯åŠ¨AIèŠå¤©**:
   \`\`\`bash
   ./start_ai.sh
   \`\`\`

2. **å¯åŠ¨Webç•Œé¢** (éœ€è¦å®‰è£…gradio):
   \`\`\`bash
   ./start_ai.sh --web
   \`\`\`

3. **æ£€æŸ¥çŽ¯å¢ƒ**:
   \`\`\`bash
   python tools.py check
   \`\`\`

## ðŸ› ï¸ å·¥å…·å‘½ä»¤

- \`./start_ai.sh\` - å¯åŠ¨AIèŠå¤©
- \`./start_ai.sh --web\` - å¯åŠ¨Webç•Œé¢
- \`./start_ai.sh --api\` - å¯åŠ¨APIæœåŠ¡
- \`python tools.py check\` - æ£€æŸ¥çŽ¯å¢ƒçŠ¶æ€
- \`python tools.py diagnose\` - è¯Šæ–­GPUé—®é¢˜
- \`python tools.py clear\` - æ¸…ç†æ¨¡åž‹ç¼“å­˜
- \`python switch_model.py\` - åˆ‡æ¢AIæ¨¡åž‹
- \`./fix_gpu.sh\` - ä¿®å¤GPUæ”¯æŒ

## ðŸ”§ è§£å†³GPUæ£€æµ‹é—®é¢˜

å¦‚æžœä½ çš„ç³»ç»Ÿæœ‰GPUä½†PyTorchæ£€æµ‹ä¸åˆ°ï¼Œè¯·è¿è¡Œ:

\`\`\`bash
./fix_gpu.sh
\`\`\`

æˆ–è€…æ‰‹åŠ¨é‡æ–°å®‰è£…GPUç‰ˆæœ¬çš„PyTorch:

\`\`\`bash
source venv/bin/activate
pip uninstall torch torchvision torchaudio -y
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
\`\`\`

## ðŸ“ æ–‡ä»¶è¯´æ˜Ž

- \`config.json\` - é…ç½®æ–‡ä»¶
- \`start_ai.sh\` - å¯åŠ¨è„šæœ¬
- \`run_ai.py\` - ä¸»ç¨‹åº
- \`tools.py\` - å·¥å…·è„šæœ¬
- \`switch_model.py\` - æ¨¡åž‹åˆ‡æ¢
- \`fix_gpu.sh\` - GPUä¿®å¤
- \`venv/\` - Pythonè™šæ‹ŸçŽ¯å¢ƒ
- \`model_cache/\` - æ¨¡åž‹ç¼“å­˜ç›®å½•
- \`model_configs/\` - æ¨¡åž‹é…ç½®ç›®å½•

## â“ å¸¸è§é—®é¢˜

### 1. é¦–æ¬¡è¿è¡Œå¾ˆæ…¢ï¼Ÿ
- é¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½æ¨¡åž‹æ–‡ä»¶
- ä¸‹è½½å®ŒæˆåŽä¼šç¼“å­˜ï¼Œä¸‹æ¬¡å¯åŠ¨æ›´å¿«
- æ¨¡åž‹å¤§å°: ${MODEL_SIZE}

### 2. GPUæœªæ£€æµ‹åˆ°ï¼Ÿ
- è¿è¡Œ: \`python tools.py diagnose\`
- è¿è¡Œ: \`./fix_gpu.sh\`
- ç¡®ä¿å·²å®‰è£…NVIDIAé©±åŠ¨å’ŒCUDA

### 3. å¦‚ä½•æ›´æ¢æ¨¡åž‹ï¼Ÿ
- è¿è¡Œ: \`python switch_model.py\`
- æˆ–å¯åŠ¨æ—¶æŒ‡å®š: \`./start_ai.sh --model "æ¨¡åž‹åç§°"\`

### 4. å†…å­˜ä¸è¶³ï¼Ÿ
- é€‰æ‹©æ›´å°çš„æ¨¡åž‹
- å…³é—­å…¶ä»–åº”ç”¨ç¨‹åº
- ç¡®ä¿æœ‰è¶³å¤Ÿçš„è™šæ‹Ÿå†…å­˜

## ðŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäºŽApache 2.0è®¸å¯è¯å¼€æº

## ðŸ¤ æ”¯æŒ

å¦‚é‡é—®é¢˜ï¼Œè¯·æ£€æŸ¥:
1. ç½‘ç»œè¿žæŽ¥æ˜¯å¦æ­£å¸¸
2. ç£ç›˜ç©ºé—´æ˜¯å¦å……è¶³
3. Pythonç‰ˆæœ¬æ˜¯å¦ä¸º3.8+
4. æŸ¥çœ‹ \`python tools.py diagnose\` çš„è¾“å‡º

EOF

# åˆ›å»ºä½¿ç”¨è¯´æ˜Žç®€ç‰ˆ
cat > QUICKSTART.txt << EOF
å¿«é€Ÿå¼€å§‹ï¼š
1. cd ai_env
2. ./start_ai.sh

å¸¸ç”¨å‘½ä»¤ï¼š
- å¯åŠ¨AI: ./start_ai.sh
- æ£€æŸ¥çŽ¯å¢ƒ: python tools.py check
- è¯Šæ–­GPU: python tools.py diagnose
- åˆ‡æ¢æ¨¡åž‹: python switch_model.py
- ä¿®å¤GPU: ./fix_gpu.sh

å®‰è£…ä¿¡æ¯ï¼š
- ç›®å½•: ${AI_HOME}
- æ¨¡åž‹: ${MODEL_NAME}
- å¤§å°: ${MODEL_SIZE}
- PyTorch: ${PYTORCH_VERSION}
- GPU: ${HAS_NVIDIA}
- CUDA: ${CUDA_VERSION:-æœªæ£€æµ‹åˆ°}
- æ—¶é—´: ${CURRENT_DATE}
EOF

# æ·»åŠ å®‰è£…å®Œæˆä¿¡æ¯
echo -e "\n${GREEN}âœ… å®‰è£…å®Œæˆï¼${NC}"
echo "=========================================="
echo "ðŸ“ å®‰è£…ç›®å½•: ${AI_HOME}"
echo "ðŸ¤– é€‰æ‹©æ¨¡åž‹: ${MODEL_NAME}"
echo "ðŸ“Š æ¨¡åž‹å¤§å°: ${MODEL_SIZE}"
echo "ðŸ”§ PyTorchç‰ˆæœ¬: ${PYTORCH_VERSION}"
if [ "$HAS_NVIDIA" = true ]; then
    echo "ðŸ–¥ï¸  æ£€æµ‹åˆ°GPU: æ˜¯"
    if [ "$HAS_CUDA" = true ] && [ -n "$CUDA_VERSION" ]; then
        echo "âš¡ CUDAç‰ˆæœ¬: ${CUDA_VERSION}.x"
    else
        echo "âš ï¸  CUDAç‰ˆæœ¬: æœªæ£€æµ‹åˆ°æˆ–ä¸å¯ç”¨"
    fi
else
    echo "ðŸ–¥ï¸  æ£€æµ‹åˆ°GPU: å¦"
fi
echo "ðŸš€ å¯åŠ¨å‘½ä»¤: ./start_ai.sh"
echo "ðŸ“– è¯¦ç»†è¯´æ˜Ž: è¯·æŸ¥çœ‹ README.md"
echo "=========================================="
echo -e "\n${BLUE}ä¸‹ä¸€æ­¥:${NC}"
echo "1. è¿›å…¥ç›®å½•: cd ${AI_HOME}"
echo "2. å¯åŠ¨AI: ./start_ai.sh"
echo "3. æ£€æŸ¥çŽ¯å¢ƒ: python tools.py check"
echo -e "\n${YELLOW}æ³¨æ„: é¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½æ¨¡åž‹ï¼ˆ${MODEL_SIZE}ï¼‰ï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚${NC}"

if [ "$HAS_NVIDIA" = true ] && [ "$PYTORCH_VERSION" = "cpu" ]; then
    echo -e "\n${RED}âš ï¸  è­¦å‘Š: æ£€æµ‹åˆ°GPUä½†é€‰æ‹©äº†CPUç‰ˆæœ¬çš„PyTorch${NC}"
    echo "ðŸ’¡ å»ºè®®: è¿è¡Œ ./fix_gpu.sh å®‰è£…GPUç‰ˆæœ¬ä»¥èŽ·å¾—æ›´å¥½çš„æ€§èƒ½"
fi