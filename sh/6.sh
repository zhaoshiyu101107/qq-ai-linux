#!/bin/bash
# AIé¡¹ç›®å®Œæ•´å®‰è£…å’Œå¯åŠ¨è„šæœ¬
# æ”¯æŒè™šæ‹Ÿç¯å¢ƒè‡ªåŠ¨åˆ›å»ºã€GPUæ£€æµ‹ã€PyTorchç‰ˆæœ¬é€‰æ‹©ã€AIæ¨¡å‹ä¸‹è½½

set -e

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
MAGENTA='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m'

# æ‰“å°åˆ†éš”çº¿
print_separator() {
    printf '=%.0s' {1..60}
    echo ""
}

# è¿›åº¦æ¡å‡½æ•°
show_progress() {
    local width=50
    local percent=$1
    local filled=$((width * percent / 100))
    local empty=$((width - filled))
    
    printf "\r["
    printf "%${filled}s" | tr ' ' '='
    printf "%${empty}s" | tr ' ' ' '
    printf "] %3d%%" $percent
}

echo -e "${GREEN}ğŸ¤– AIé¡¹ç›®å¯åŠ¨${NC}"
print_separator

# è·å–é¡¹ç›®è·¯å¾„
PROJECT_PATH="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
echo "é¡¹ç›®ç›®å½•: $PROJECT_PATH"

# è™šæ‹Ÿç¯å¢ƒè·¯å¾„
VENV_PATH="${PROJECT_PATH}/venv"

# æ£€æŸ¥è„šæœ¬å‚æ•°
INSTALL_MODE=false
CHECK_ONLY=false
WEB_MODE=false
API_MODE=false
MODEL_OVERRIDE=""

while [[ $# -gt 0 ]]; do
    case $1 in
        --install)
            INSTALL_MODE=true
            shift
            ;;
        --check)
            CHECK_ONLY=true
            shift
            ;;
        --web)
            WEB_MODE=true
            shift
            ;;
        --api)
            API_MODE=true
            shift
            ;;
        --model)
            MODEL_OVERRIDE="$2"
            shift 2
            ;;
        --help)
            echo "ç”¨æ³•: $0 [é€‰é¡¹]"
            echo "é€‰é¡¹:"
            echo "  --install     é‡æ–°å®‰è£…ç¯å¢ƒå’Œä¾èµ–"
            echo "  --check       ä»…æ£€æŸ¥ç¯å¢ƒ"
            echo "  --web         å¯åŠ¨Webç•Œé¢"
            echo "  --api         å¯åŠ¨APIæœåŠ¡"
            echo "  --model <name> æŒ‡å®šè¦ä½¿ç”¨çš„æ¨¡å‹"
            echo "  --help        æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯"
            exit 0
            ;;
        *)
            echo "æœªçŸ¥é€‰é¡¹: $1"
            echo "ä½¿ç”¨ --help æŸ¥çœ‹å¸®åŠ©"
            exit 1
            ;;
    esac
done

# æ£€æŸ¥å¹¶åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
if [ ! -f "$VENV_PATH/bin/activate" ] || [ "$INSTALL_MODE" = true ]; then
    if [ ! -f "$VENV_PATH/bin/activate" ]; then
        echo -e "${YELLOW}è™šæ‹Ÿç¯å¢ƒä¸å­˜åœ¨: $VENV_PATH${NC}"
    else
        echo -e "${YELLOW}é‡æ–°å®‰è£…æ¨¡å¼ï¼Œå°†é‡æ–°è®¾ç½®è™šæ‹Ÿç¯å¢ƒ${NC}"
    fi
    
    # è¯¢é—®ç”¨æˆ·ç¡®è®¤
    if [ ! -f "$VENV_PATH/bin/activate" ]; then
        read -p "æ˜¯å¦åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼Ÿ(y/n, é»˜è®¤: y): " create_venv
        create_venv=${create_venv:-y}
    else
        read -p "å°†é‡æ–°åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼Œç°æœ‰ç¯å¢ƒå°†è¢«è¦†ç›–ã€‚ç»§ç»­ï¼Ÿ(y/n, é»˜è®¤: n): " create_venv
        create_venv=${create_venv:-n}
    fi
    
    if [[ $create_venv == "y" || $create_venv == "Y" ]]; then
        # æ£€æŸ¥Python3æ˜¯å¦å¯ç”¨
        if ! command -v python3 &> /dev/null; then
            echo -e "${RED}âŒ æœªæ‰¾åˆ°python3ï¼Œè¯·å…ˆå®‰è£…Python3ã€‚${NC}"
            echo "è®¿é—® https://www.python.org/downloads/ è·å–å®‰è£…åŒ…"
            exit 1
        fi
        
        # åˆ é™¤ç°æœ‰è™šæ‹Ÿç¯å¢ƒ
        if [ -d "$VENV_PATH" ]; then
            rm -rf "$VENV_PATH"
        fi
        
        echo -n "åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ..."
        python3 -m venv "$VENV_PATH" 2>/dev/null &
        python_pid=$!
        
        # æ˜¾ç¤ºç®€å•è¿›åº¦æ¡
        for i in {1..20}; do
            show_progress $((i*5))
            sleep 0.2
            if ! kill -0 $python_pid 2>/dev/null; then
                break
            fi
        done
        show_progress 100
        echo ""
        
        wait $python_pid
        
        if [ $? -eq 0 ]; then
            echo -e "${GREEN}âœ… è™šæ‹Ÿç¯å¢ƒåˆ›å»ºæˆåŠŸ: $VENV_PATH${NC}"
        else
            echo -e "${RED}âŒ è™šæ‹Ÿç¯å¢ƒåˆ›å»ºå¤±è´¥${NC}"
            echo "å¯èƒ½éœ€è¦å®‰è£…python3-venvåŒ…:"
            echo "Ubuntu/Debian: sudo apt-get install python3-venv"
            echo "CentOS/RHEL: sudo yum install python3-venv"
            echo "macOS: ç¡®ä¿å·²å®‰è£…Python3"
            exit 1
        fi
    else
        echo -e "${YELLOW}âŒ å–æ¶ˆåˆ›å»ºè™šæ‹Ÿç¯å¢ƒ${NC}"
        if [ ! -f "$VENV_PATH/bin/activate" ]; then
            exit 1
        fi
    fi
fi

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
echo "æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ..."
source "$VENV_PATH/bin/activate"

# åœ¨å®‰è£…æ¨¡å¼ä¸‹å‡çº§pipå’Œå®‰è£…åŸºç¡€ä¾èµ–
if [ ! -f "$VENV_PATH/.installed" ] || [ "$INSTALL_MODE" = true ]; then
    echo -e "${YELLOW}[1/4] å‡çº§pip...${NC}"
    echo -n "å‡çº§pip..."
    pip install --upgrade pip -q 2>/dev/null &
    pip_pid=$!
    
    for i in {1..20}; do
        show_progress $((i*5))
        sleep 0.1
        if ! kill -0 $pip_pid 2>/dev/null; then
            break
        fi
    done
    show_progress 100
    echo ""
    wait $pip_pid
    
    echo -e "${YELLOW}[2/4] æ£€æµ‹ç³»ç»Ÿç¯å¢ƒ...${NC}"
    
    # æ£€æµ‹GPUå’ŒCUDA
    HAS_NVIDIA=false
    HAS_CUDA=false
    CUDA_VERSION=""
    
    # æ£€æŸ¥nvidia-smiå‘½ä»¤
    if command -v nvidia-smi &> /dev/null; then
        HAS_NVIDIA=true
        echo -e "${GREEN}âœ“ æ£€æµ‹åˆ°NVIDIA GPU${NC}"
        
        # æ£€æŸ¥CUDAç‰ˆæœ¬
        if nvidia-smi --query | grep -q "CUDA Version"; then
            HAS_CUDA=true
            CUDA_VERSION=$(nvidia-smi --query | grep "CUDA Version" | awk '{print $NF}' | cut -d'.' -f1)
            echo -e "${GREEN}âœ“ æ£€æµ‹åˆ°CUDAç‰ˆæœ¬: ${CUDA_VERSION}${NC}"
        elif nvidia-smi | grep -q "CUDA Version"; then
            HAS_CUDA=true
            CUDA_VERSION=$(nvidia-smi | grep "CUDA Version" | awk '{print $NF}' | cut -d'.' -f1)
            echo -e "${GREEN}âœ“ æ£€æµ‹åˆ°CUDAç‰ˆæœ¬: ${CUDA_VERSION}${NC}"
        else
            echo -e "${YELLOW}âš  NVIDIAé©±åŠ¨å·²å®‰è£…ä½†æœªæ£€æµ‹åˆ°CUDAç‰ˆæœ¬${NC}"
            # å°è¯•ä»nvccè·å–CUDAç‰ˆæœ¬
            if command -v nvcc &> /dev/null; then
                CUDA_VERSION=$(nvcc --version | grep "release" | awk '{print $6}' | cut -c1-2)
                echo -e "${GREEN}âœ“ ä»nvccæ£€æµ‹åˆ°CUDAç‰ˆæœ¬: ${CUDA_VERSION}${NC}"
                HAS_CUDA=true
            fi
        fi
    else
        echo -e "${YELLOW}â„¹ æœªæ£€æµ‹åˆ°NVIDIA GPUæˆ–é©±åŠ¨${NC}"
    fi
    
    # è¯¢é—®ç”¨æˆ·é€‰æ‹©PyTorchç‰ˆæœ¬
    echo -e "\n${BLUE}è¯·é€‰æ‹©PyTorchå®‰è£…ç‰ˆæœ¬:${NC}"
    
    if [ "$HAS_NVIDIA" = true ] && [ "$HAS_CUDA" = true ]; then
        echo "1. GPUåŠ é€Ÿç‰ˆ (CUDA ${CUDA_VERSION}.x) - æ¨èï¼Œéœ€è¦NVIDIA GPU"
        echo "2. CPUç‰ˆ - é€šç”¨å…¼å®¹ï¼Œæ— GPUåŠ é€Ÿ"
        echo "3. è‡ªåŠ¨é€‰æ‹© - æ ¹æ®ç³»ç»Ÿè‡ªåŠ¨é€‰æ‹©æœ€ä½³ç‰ˆæœ¬"
        
        read -p "è¯·é€‰æ‹© [1/2/3] (é»˜è®¤: 3): " choice
        
        case ${choice:-3} in
            1)
                PYTORCH_VERSION="gpu"
                echo -e "${YELLOW}é€‰æ‹©: GPUåŠ é€Ÿç‰ˆæœ¬${NC}"
                ;;
            2)
                PYTORCH_VERSION="cpu"
                echo -e "${YELLOW}é€‰æ‹©: CPUç‰ˆæœ¬${NC}"
                ;;
            3)
                PYTORCH_VERSION="auto"
                echo -e "${YELLOW}é€‰æ‹©: è‡ªåŠ¨é€‰æ‹©ç‰ˆæœ¬${NC}"
                ;;
        esac
    else
        echo "1. CPUç‰ˆ - å”¯ä¸€å¯ç”¨é€‰é¡¹"
        echo "2. è‡ªåŠ¨é€‰æ‹© - æ ¹æ®ç³»ç»Ÿè‡ªåŠ¨é€‰æ‹©"
        read -p "è¯·é€‰æ‹© [1/2] (é»˜è®¤: 2): " choice
        
        if [ "${choice:-2}" = "1" ]; then
            PYTORCH_VERSION="cpu"
            echo -e "${YELLOW}é€‰æ‹©: CPUç‰ˆæœ¬${NC}"
        else
            PYTORCH_VERSION="auto"
            echo -e "${YELLOW}é€‰æ‹©: è‡ªåŠ¨é€‰æ‹©ç‰ˆæœ¬${NC}"
        fi
    fi
    
    # æ ¹æ®é€‰æ‹©å®‰è£…PyTorch
    echo -e "\n${YELLOW}[3/4] å®‰è£…PyTorch...${NC}"
    
    if [ "$PYTORCH_VERSION" = "auto" ]; then
        if [ "$HAS_NVIDIA" = true ] && [ "$HAS_CUDA" = true ]; then
            echo "è‡ªåŠ¨é€‰æ‹©: å®‰è£…GPUç‰ˆæœ¬ (CUDA ${CUDA_VERSION}.x)"
            PYTORCH_VERSION="gpu"
        else
            echo "è‡ªåŠ¨é€‰æ‹©: å®‰è£…CPUç‰ˆæœ¬"
            PYTORCH_VERSION="cpu"
        fi
    fi
    
    case $PYTORCH_VERSION in
        "cpu")
            echo "å®‰è£…PyTorch CPUç‰ˆæœ¬..."
            pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu > /dev/null 2>&1
            ;;
        "gpu")
            echo "å®‰è£…PyTorch GPUç‰ˆæœ¬..."
            
            # æ ¹æ®æ£€æµ‹åˆ°çš„CUDAç‰ˆæœ¬é€‰æ‹©å¯¹åº”çš„PyTorch
            if [ -n "$CUDA_VERSION" ]; then
                if [ "$CUDA_VERSION" = "12" ] || [ "$CUDA_VERSION" -ge 12 ]; then
                    echo "ä½¿ç”¨CUDA 12.1ç‰ˆæœ¬..."
                    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 > /dev/null 2>&1
                elif [ "$CUDA_VERSION" = "11" ]; then
                    echo "ä½¿ç”¨CUDA 11.8ç‰ˆæœ¬..."
                    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 > /dev/null 2>&1
                else
                    echo "ä½¿ç”¨é»˜è®¤CUDAç‰ˆæœ¬ (11.8)..."
                    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 > /dev/null 2>&1
                fi
            else
                echo "ä½¿ç”¨é»˜è®¤CUDAç‰ˆæœ¬ (11.8)..."
                pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 > /dev/null 2>&1
            fi
            ;;
    esac
    
    # å®‰è£…AIåº“
    echo -e "\n${YELLOW}[4/4] å®‰è£…AIåº“...${NC}"
    echo "å®‰è£…Transformerså’Œå…¶ä»–AIåº“..."
    pip install transformers accelerate sentencepiece protobuf einops tiktoken > /dev/null 2>&1
    
    # æ ‡è®°ä¸ºå·²å®‰è£…
    touch "$VENV_PATH/.installed"
    
    # ä¿å­˜é…ç½®ä¿¡æ¯
    cat > "$VENV_PATH/config.json" << EOF
{
  "install_date": "$(date '+%Y-%m-%d %H:%M:%S')",
  "pytorch_version": "${PYTORCH_VERSION}",
  "has_nvidia": ${HAS_NVIDIA},
  "has_cuda": ${HAS_CUDA},
  "cuda_version": "${CUDA_VERSION:-null}"
}
EOF
    
    echo -e "${GREEN}âœ… æ‰€æœ‰ä¾èµ–å®‰è£…å®Œæˆï¼${NC}"
fi

# æ£€æŸ¥PyTorchæ˜¯å¦æ­£å¸¸å·¥ä½œ
echo -n "æ£€æŸ¥PyTorchå®‰è£…çŠ¶æ€..."
if ! python -c "import torch" 2>/dev/null; then
    echo -e "\n${RED}âŒ PyTorchæœªæ­£ç¡®å®‰è£…${NC}"
    echo "æ­£åœ¨å°è¯•é‡æ–°å®‰è£…PyTorch..."
    
    if command -v nvidia-smi &> /dev/null; then
        echo "æ£€æµ‹åˆ°NVIDIA GPUï¼Œå®‰è£…GPUç‰ˆæœ¬PyTorch..."
        pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 --no-cache-dir
    else
        echo "å®‰è£…CPUç‰ˆæœ¬PyTorch..."
        pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu --no-cache-dir
    fi
else
    echo -e "${GREEN}âœ… PyTorchå·²å®‰è£…${NC}"
fi

# æ£€æŸ¥GPUçŠ¶æ€
echo "æ£€æŸ¥GPUçŠ¶æ€..."
if python -c "import torch" 2>/dev/null; then
    python -c "
import torch
print(f'PyTorchç‰ˆæœ¬: {torch.__version__}')
print(f'CUDAå¯ç”¨: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'ğŸ® GPUåŠ é€Ÿå·²å¯ç”¨ ({torch.cuda.device_count()}ä¸ªGPU)')
    for i in range(torch.cuda.device_count()):
        print(f'  GPU {i}: {torch.cuda.get_device_name(i)}')
else:
    print('âš ï¸  ä½¿ç”¨CPUè¿è¡Œï¼Œé€Ÿåº¦è¾ƒæ…¢')
"
else
    echo -e "${RED}âŒ PyTorchæœªæ­£ç¡®å®‰è£…${NC}"
    exit 1
fi

# å¦‚æœåªéœ€è¦æ£€æŸ¥ç¯å¢ƒ
if [ "$CHECK_ONLY" = true ]; then
    print_separator
    echo "ç¯å¢ƒæ£€æŸ¥å®Œæˆï¼"
    exit 0
fi

# è¯¢é—®ç”¨æˆ·é€‰æ‹©æ¨¡å‹ï¼ˆå¦‚æœæœªé€šè¿‡å‚æ•°æŒ‡å®šä¸”æœªå®‰è£…è¿‡ï¼‰
MODEL_CONFIG="${PROJECT_PATH}/model_config.json"
if [ -z "$MODEL_OVERRIDE" ] && [ ! -f "$MODEL_CONFIG" ]; then
    echo -e "\n${BLUE}è¯·é€‰æ‹©è¦ä½¿ç”¨çš„AIæ¨¡å‹:${NC}"
    echo "1. Qwen/Qwen3-0.5B-Instruct (è½»é‡çº§, çº¦0.5GB)"
    echo "2. Qwen/Qwen3-0.6B-Instruct (æ¨è, çº¦1.2GB)"
    echo "3. Qwen/Qwen3-1.8B-Instruct (å¹³è¡¡, çº¦3.6GB)"
    echo "4. Qwen/Qwen2.5-0.5B-Instruct (æ–°ç‰ˆ, çº¦0.5GB)"
    echo "5. microsoft/phi-2 (å¾®è½¯Phi-2, çº¦2.7GB)"
    echo "6. TinyLlama/TinyLlama-1.1B-Chat-v1.0 (å°ç¾Šé©¼, çº¦2.2GB)"
    echo "7. è‡ªå®šä¹‰æ¨¡å‹ (è¾“å…¥å®Œæ•´çš„HuggingFaceæ¨¡å‹è·¯å¾„)"
    
    read -p "è¯·é€‰æ‹© [1-7] (é»˜è®¤: 2): " model_choice
    
    case ${model_choice:-2} in
        1)
            MODEL_NAME="Qwen/Qwen3-0.5B-Instruct"
            MODEL_SIZE="çº¦0.5GB"
            ;;
        3)
            MODEL_NAME="Qwen/Qwen3-1.8B-Instruct"
            MODEL_SIZE="çº¦3.6GB"
            ;;
        4)
            MODEL_NAME="Qwen/Qwen2.5-0.5B-Instruct"
            MODEL_SIZE="çº¦0.5GB"
            ;;
        5)
            MODEL_NAME="microsoft/phi-2"
            MODEL_SIZE="çº¦2.7GB"
            ;;
        6)
            MODEL_NAME="TinyLlama/TinyLlama-1.1B-Chat-v1.0"
            MODEL_SIZE="çº¦2.2GB"
            ;;
        7)
            read -p "è¯·è¾“å…¥å®Œæ•´çš„HuggingFaceæ¨¡å‹è·¯å¾„: " custom_model
            if [ -n "$custom_model" ]; then
                MODEL_NAME="$custom_model"
                MODEL_SIZE="æœªçŸ¥å¤§å°"
                echo -e "${YELLOW}ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹: ${MODEL_NAME}${NC}"
            else
                MODEL_NAME="Qwen/Qwen3-0.6B-Instruct"
                MODEL_SIZE="çº¦1.2GB"
                echo -e "${YELLOW}ä½¿ç”¨é»˜è®¤æ¨¡å‹: ${MODEL_NAME}${NC}"
            fi
            ;;
        2)
            MODEL_NAME="Qwen/Qwen3-0.6B-Instruct"
            MODEL_SIZE="çº¦1.2GB"
            ;;
    esac
    
    # ä¿å­˜æ¨¡å‹é…ç½®
    cat > "$MODEL_CONFIG" << EOF
{
  "model": "${MODEL_NAME}",
  "model_size": "${MODEL_SIZE}",
  "selected_date": "$(date '+%Y-%m-%d %H:%M:%S')"
}
EOF
elif [ -n "$MODEL_OVERRIDE" ]; then
    # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šçš„æ¨¡å‹
    MODEL_NAME="$MODEL_OVERRIDE"
    MODEL_SIZE="æœªçŸ¥å¤§å°"
    
    cat > "$MODEL_CONFIG" << EOF
{
  "model": "${MODEL_NAME}",
  "model_size": "${MODEL_SIZE}",
  "selected_date": "$(date '+%Y-%m-%d %H:%M:%S')"
}
EOF
    echo -e "${YELLOW}ä½¿ç”¨æŒ‡å®šæ¨¡å‹: ${MODEL_NAME}${NC}"
else
    # è¯»å–ç°æœ‰é…ç½®
    if [ -f "$MODEL_CONFIG" ]; then
        MODEL_NAME=$(python -c "import json; print(json.load(open('$MODEL_CONFIG'))['model'])")
        MODEL_SIZE=$(python -c "import json; print(json.load(open('$MODEL_CONFIG'))['model_size'])")
        echo -e "${GREEN}ä½¿ç”¨å·²é…ç½®æ¨¡å‹: ${MODEL_NAME}${NC}"
    else
        MODEL_NAME="Qwen/Qwen3-0.6B-Instruct"
        MODEL_SIZE="çº¦1.2GB"
    fi
fi

# è¿›å…¥é¡¹ç›®ç›®å½•
cd "$PROJECT_PATH"

print_separator
echo "é¡¹ç›®ç›®å½•: $PROJECT_PATH"
echo "è™šæ‹Ÿç¯å¢ƒ: $VENV_PATH"
echo "é€‰æ‹©æ¨¡å‹: $MODEL_NAME ($MODEL_SIZE)"
print_separator

# åˆ›å»ºä¸»è¿è¡Œè„šæœ¬ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
if [ ! -f "run_ai.py" ]; then
    echo "åˆ›å»ºä¸»è¿è¡Œè„šæœ¬..."
    cat > run_ai.py << 'EOF'
#!/usr/bin/env python3
"""
AIæ¨¡å‹æœ¬åœ°è¿è¡Œè„šæœ¬
æ”¯æŒå¤šç§æ¨¡å‹ï¼Œå®Œå…¨æœ¬åœ°è¿è¡Œ
"""

import os
import sys
import torch
import json
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = os.path.join(os.path.dirname(__file__), "model_config.json")
    if os.path.exists(config_path):
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}

def check_gpu_info():
    """æ£€æŸ¥GPUä¿¡æ¯"""
    config = load_config()
    
    print("=" * 50)
    print("ğŸ¤– æœ¬åœ°AIç¯å¢ƒ")
    print("=" * 50)
    print(f"ç›®å½•: {os.path.dirname(os.path.abspath(__file__))}")
    print(f"æ¨¡å‹: {config.get('model', 'Qwen/Qwen3-0.6B-Instruct')}")
    print(f"æ¨¡å‹å¤§å°: {config.get('model_size', 'çº¦1.2GB')}")
    print(f"Python: {sys.version.split()[0]}")
    print(f"PyTorch: {torch.__version__}")
    
    # æ£€æŸ¥CUDA
    cuda_available = torch.cuda.is_available()
    print(f"CUDAå¯ç”¨: {cuda_available}")
    
    if cuda_available:
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
    else:
        print("âš  ä½¿ç”¨CPUæ¨¡å¼ - é€Ÿåº¦è¾ƒæ…¢")
    
    print("=" * 50)
    print()

def load_model():
    """åŠ è½½AIæ¨¡å‹"""
    from transformers import AutoTokenizer, AutoModelForCausalLM
    
    config = load_config()
    model_name = config.get('model', 'Qwen/Qwen3-0.6B-Instruct')
    model_size = config.get('model_size', 'çº¦1.2GB')
    
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")
    print(f"æ¨¡å‹å¤§å°: {model_size}")
    print("é¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½æ¨¡å‹æ–‡ä»¶")
    print("ä¸‹è½½å®Œæˆåä¼šç¼“å­˜ï¼Œä¸‹æ¬¡æ— éœ€é‡æ–°ä¸‹è½½")
    print()
    
    # è®¾ç½®æ¨¡å‹ç¼“å­˜ç›®å½•åˆ°æœ¬åœ°
    cache_dir = os.path.join(os.path.dirname(__file__), "model_cache")
    os.makedirs(cache_dir, exist_ok=True)
    
    try:
        # åŠ è½½tokenizer
        print("1. åŠ è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            trust_remote_code=True
        )
        
        # åŠ è½½æ¨¡å‹
        print("2. åŠ è½½æ¨¡å‹...")
        
        # æ ¹æ®æ˜¯å¦æœ‰GPUé€‰æ‹©åŠ è½½æ–¹å¼
        if torch.cuda.is_available():
            print("  ä½¿ç”¨GPUåŠ é€Ÿ")
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16,
                device_map="auto",
                cache_dir=cache_dir,
                trust_remote_code=True
            )
        else:
            print("  ä½¿ç”¨CPUè¿è¡Œ")
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float32,
                device_map="cpu",
                cache_dir=cache_dir,
                trust_remote_code=True
            )
        
        return tokenizer, model
        
    except Exception as e:
        print(f"\nâŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        return None, None

def chat_loop(tokenizer, model):
    """å¯¹è¯å¾ªç¯"""
    print()
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    print("-" * 50)
    print("ğŸ’¡ æç¤º: è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
    print("-" * 50)
    
    while True:
        try:
            user_input = input("\nä½ : ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'q']:
                print("å†è§ï¼ğŸ‘‹")
                break
                
            if not user_input:
                continue
            
            # å‡†å¤‡å¯¹è¯æ ¼å¼
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„AIåŠ©æ‰‹ã€‚"},
                {"role": "user", "content": user_input}
            ]
            
            try:
                text = tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
            except:
                # å¦‚æœæ¨¡å‹ä¸æ”¯æŒchat_templateï¼Œä½¿ç”¨ç®€å•æ ¼å¼
                text = f"ç”¨æˆ·: {user_input}\nåŠ©æ‰‹:"
            
            # ç¼–ç å¹¶ç”Ÿæˆ
            model_inputs = tokenizer(text, return_tensors="pt").to(model.device)
            
            print("æ€è€ƒä¸­...", end="", flush=True)
            
            with torch.no_grad():
                outputs = model.generate(
                    **model_inputs,
                    max_new_tokens=200,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            print("\r" + " " * 20, end="\r")  # æ¸…é™¤"æ€è€ƒä¸­..."
            
            # è§£ç å›å¤
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # æå–æ¨¡å‹å›å¤éƒ¨åˆ†
            if "åŠ©æ‰‹:" in response:
                response = response.split("åŠ©æ‰‹:")[-1].strip()
            elif "assistant" in response:
                response = response.split("assistant")[-1].strip()
            elif "Assistant:" in response:
                response = response.split("Assistant:")[-1].strip()
            
            print(f"AI: {response}")
            
        except KeyboardInterrupt:
            print("\n\né€€å‡ºç¨‹åº")
            break
        except Exception as e:
            print(f"\né”™è¯¯: {e}")
            continue

def main():
    import argparse
    parser = argparse.ArgumentParser(description="è¿è¡Œæœ¬åœ°AIæ¨¡å‹")
    parser.add_argument("--web", action="store_true", help="å¯åŠ¨Webç•Œé¢")
    parser.add_argument("--api", action="store_true", help="å¯åŠ¨APIæœåŠ¡")
    
    args = parser.parse_args()
    
    check_gpu_info()
    
    # åŠ è½½æ¨¡å‹
    tokenizer, model = load_model()
    
    if tokenizer is None or model is None:
        print("\nğŸ’¡ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
        print("1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("2. ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´")
        print("3. æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®")
        return
    
    if args.web:
        # å¯åŠ¨Webç•Œé¢
        try:
            import gradio as gr
            print("å¯åŠ¨Webç•Œé¢...")
            
            def respond(message, history):
                inputs = tokenizer(message, return_tensors="pt").to(model.device)
                with torch.no_grad():
                    outputs = model.generate(**inputs, max_new_tokens=200)
                response = tokenizer.decode(outputs[0], skip_special_tokens=True)
                return response
            
            gr.ChatInterface(respond).launch(server_name="0.0.0.0", server_port=7860)
        except ImportError:
            print("æœªå®‰è£…gradioï¼Œæ— æ³•å¯åŠ¨Webç•Œé¢")
            print("è¯·è¿è¡Œ: pip install gradio")
            chat_loop(tokenizer, model)
    elif args.api:
        # å¯åŠ¨APIæœåŠ¡
        try:
            from fastapi import FastAPI
            import uvicorn
            print("å¯åŠ¨APIæœåŠ¡...")
            
            app = FastAPI()
            
            @app.post("/chat")
            async def chat_endpoint(message: dict):
                user_input = message.get("message", "")
                inputs = tokenizer(user_input, return_tensors="pt").to(model.device)
                with torch.no_grad():
                    outputs = model.generate(**inputs, max_new_tokens=200)
                response = tokenizer.decode(outputs[0], skip_special_tokens=True)
                return {"response": response}
            
            uvicorn.run(app, host="0.0.0.0", port=8000)
        except ImportError:
            print("æœªå®‰è£…fastapiå’Œuvicornï¼Œæ— æ³•å¯åŠ¨APIæœåŠ¡")
            print("è¯·è¿è¡Œ: pip install fastapi uvicorn")
            chat_loop(tokenizer, model)
    else:
        # å¯åŠ¨äº¤äº’å¼èŠå¤©
        chat_loop(tokenizer, model)

if __name__ == "__main__":
    main()
EOF
fi

print_separator
echo -e "${GREEN}ğŸš€ å¯åŠ¨AIåº”ç”¨...${NC}"
echo "è¾“å…¥ 'quit' é€€å‡ºåº”ç”¨"
print_separator

# è¿è¡Œä¸»ç¨‹åº
ARGS=""
if [ "$WEB_MODE" = true ]; then
    ARGS="$ARGS --web"
elif [ "$API_MODE" = true ]; then
    ARGS="$ARGS --api"
fi

python run_ai.py $ARGS